Title,Link,Additional Resource 1,Additional Resource 2,Additional Resource 3,Venue,Year,Abstract,Authors,
"Too Many Notes: Computers, Complexity and Culture in ""Voyager""",https://www.jstor.org/stable/1513376,https://www.youtube.com/watch?v=hO47LiHsFtc&list=RDhO47LiHsFtc&start_radio=1 ,https://www.youtube.com/watch?v=ncy4_FHX3Jc ,,Leonardo Music Journal,2000,"Recent advances in generative artificial intelligence (AI) have created models capable of high-quality musical content generation. However, little consideration is given to how to use these models for real-time or cooperative jamming musical applications because of crucial required features: low latency, the ability to communicate planned actions, and the ability to adapt to user input in real-time. To support these needs, we introduce ReaLJam, an interface and protocol for live musical jamming sessions between a human and a Transformer-based AI agent trained with reinforcement learning. We enable real-time interactions using the concept of anticipation, where the agent continually predicts how the performance will unfold and visually conveys its plan to the user. We conduct a user study where experienced musicians jam in real-time with the agent through ReaLJam. Our results demonstrate that ReaLJam enables enjoyable and musically interesting sessions, and we uncover important takeaways for future work.","Alexander Scarlatos, Yusong Wu, Ian Simon, Adam Roberts, Tim Cooijmans, Natasha Jaques, Cassie Tarakajian, Anna Huang",
Reflexive Loopers for Solo Musical Improvisation,https://research.gold.ac.uk/id/eprint/9888/1/Reflexive%20Loopers%20for%20Solo%20Music%20Improvisation.pdf,https://www.youtube.com/watch?v=Xp8tixrPM1U ,,,CHI,2013,"Loop pedals are real-time samplers that playback audio played previously by a musician. Such pedals are routinely used for music practice or outdoor ""busking"". However, loop pedals always playback the same material, which can make performances monotonous and boring both to the musician and the audience, preventing their widespread uptake in professional concerts. In response, we propose a new approach to loop pedals that addresses this issue, which is based on an analytical multi-modal representation of the audio input. Instead of simply playing back prerecorded audio, our system enables real-time generation of an audio accompaniment reacting to what is currently being performed by the musician. By combining different modes of performance - e.g. bass line, chords, solo - from the musician and system automatically, solo musicians can perform duets or trios with themselves, without engendering the so-called canned (boringly repetitive and unresponsive) music effect of loop pedals. We describe the technology, based on supervised classification and concatenative synthesis, and then illustrate our approach on solo performances of jazz standards by guitar. We claim this approach opens up new avenues for concert performance.","François Pachet, Pierre Roy, Julian Moreira, Mark d'Inverno",
Shimon: an interactive improvisational robotic marimba player,https://dl.acm.org/doi/pdf/10.1145/1753846.1753925,https://ieeexplore.ieee.org/document/5509182,https://www.youtube.com/watch?v=0dOn-EvSPUs  ,https://www.youtube.com/watch?v=l9OUbqWHOSk   ,CHI EA,2010,"Shimon is an autonomous marimba-playing robot designed to create interactions with human players that lead to novel musical outcomes. The robot combines music perception, interaction, and improvisation with the capacity to produce melodic and harmonic acoustic responses through choreographic gestures. We developed an anticipatory action framework, and a gesture-based behavior system, allowing the robot to play improvised Jazz with humans in synchrony, fluently, and without delay. In addition, we built an expressive non-humanoid head for musical social communication. This paper describes our system, used in a performance and demonstration at the CHI 2010 Media Showcase.","Guy Hoffman, Gil Weinberg",
Piano Genie,https://arxiv.org/pdf/1810.05246,,,,IUI,2019,"We present Piano Genie, an intelligent controller which allows non-musicians to improvise on the piano. With Piano Genie, a user performs on a simple interface with eight buttons, and their performance is decoded into the space of plausible piano music in real time. To learn a suitable mapping procedure for this problem, we train recurrent neural network autoencoders with discrete bottlenecks: an encoder learns an appropriate sequence of buttons corresponding to a piano piece, and a decoder learns to map this sequence back to the original piece. During performance, we substitute a user's input for the encoder output, and play the decoder's prediction each time the user presses a button. To improve the intuitiveness of Piano Genie's performance behavior, we impose musically meaningful constraints over the encoder's outputs.","Chris Donahue, Ian Simon, Sander Dieleman",
BachDuet: A Deep Learning System for Human-Machine Counterpoint Improvisation,https://www.nime.org/proceedings/2020/nime2020_paper125.pdf,,,,NIME,2020,"During theBaroque period, improvisation was a key element of music performance and education. Great musicians, such as J.S. Bach, were better known as improvisers than composers. Today, however, there is a lack of improvisation culture in classical music performance and education; classical musicians either are not trained to improvise, or cannot find other people to improvise with. Motivated by this observation, we develop BachDuet, a system that enables real-time counterpoint improvisation between a human anda machine. This system uses a recurrent neural network toprocess the human musicianâs monophonic performance ona MIDI keyboard and generates the machineâs monophonic performance in real time. We develop a GUI to visualize the generated music content and to facilitate this interaction. We conduct user studies with 13 musically trained users and show the feasibility of two-party duet counterpoint improvisation and the effectiveness of BachDuet for this purpose. We also conduct listening tests with 48 participants and show that they cannot tell the difference between duets generated by human-machine improvisation using BachDuet and those generated by human-human improvisation. Objective evaluation is also conducted to assess the degree to which these improvisations adhere to common rules of counterpoint, showing promising results.","Christodoulos Benetatos, Joseph VanderStel, and Zhiyao Duan",
Adaptive Accompaniment with ReaLchords,https://arxiv.org/pdf/2506.14723,https://arxiv.org/pdf/2502.21267,,,ICML,2024,"Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an online manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.","Yusong Wu, Tim Cooijmans, Kyle Kastner, Adam Roberts, Ian Simon, Alexander Scarlatos, Chris Donahue, Cassie Tarakajian, Shayegan Omidshafiei, Aaron Courville, Pablo Samuel Castro, Natasha Jaques, Cheng-Zhi Anna Huang",
Spire Muse:  A virtual musical partner for creative brainstorming,https://nime.pubpub.org/pub/wcj8sjee/release/1,https://nime.org/proceedings/2023/nime2023_35.pdf,,,NIME,2021,"We present Spire Muse, a co-creative musical agent that engages in different kinds of interactive behaviors. The software utilizes corpora of solo instrumental performances encoded as self-organized maps and outputs slices of the corpora as concatenated, remodeled audio sequences. Transitions between behaviors can be automated, and the interface enables the negotiation of these transitions through feedback buttons that signal approval, force reversions to previous behaviors, or request change. Musical responses are embedded in a pre-trained latent space, emergent in the interaction, and influenced through the weighting of rhythmic, spectral, harmonic, and melodic features. The training and run-time modules utilize a modified version of the MASOM agent architecture. Our model stimulates spontaneous creativity and reduces the need for the user to sustain analytical mind frames, thereby optimizing flow. The agent traverses a system autonomy axis ranging from reactive to proactive, which includes the behaviors of shadowing, mirroring, and coupling. A fourth behavior—negotiation—is emergent from the interface between agent and user. The synergy of corpora, interactive modes, and influences induces musical responses along a musical similarity axis from converging to diverging. We share preliminary observations from experiments with the agent and discuss design challenges and future prospects.","Notto J. W. Thelle, Philippe Pasquier",
GenJam: A Genetic Algorithm for Generating Jazz Solos,https://quod.lib.umich.edu/i/icmc/bbp2372.1994.033/4/--genjam-a-genetic-algorithm-for-generating-jazz-solos?page=root;size=50;view=text,,,,ICMC,1994,"This paper describes GenJam, a genetic algorithm-based model of a novice jazz musician learning to improvise. GenJam maintains hierarchically related populations of melodic ideas that are mapped to specific notes through scales suggested by the chord progression being played. As GenJam plays its solos over the accompaniment of a standard rhythm section, a human mentor gives real-time feedback, which is used to derive fitness values for the individual measures and phrases. GenJam then applies various genetic operators to the populations to breed improved generations of ideas.",John A. Biles,
Cocreative Interaction: Somax2 and the REACH Project,https://direct.mit.edu/comj/article-abstract/46/4/7/119103/Cocreative-Interaction-Somax2-and-the-REACH?redirectedFrom=PDF,,,,Computer Music Journal,2022,"Somax2 is an artificial intelligence (AI)-based multiagent system for human–machine “coimprovisation” that generates stylistically coherent streams while continuously listening and adapting to musicians or other agents. The model on which it is based can be used with little configuration to interact with humans in full autonomy, but it also allows fine real-time control of its generative processes and interaction strategies, closer in this case to a “smart” digital instrument. An offspring of the Omax system, conceived at the Institut de Recherche et Coordination Acoustique/Musique (IRCAM), the Somax2 environment is part of the European Research Council Raising Cocreativity in Cyber–Human Musicianship (REACH) project, which studies distributed creativity as a general template for symbiotic interaction between humans and digital systems. It fosters mixed musical reality involving cocreative AI agents. The REACH project puts forward the idea that cocreativity in cyber–human systems results from the emergence of complex joint behavior, produced by interaction and featuring cross-learning mechanisms. Somax2 is a first step toward this ideal, and already shows life-size achievements. This article describes Somax2 extensively, from its theoretical model to its system architecture, through its listening and learning strategies, representation spaces, and interaction policies.","Gérard Assayag, Laurent Bonnasse-Gahot, Joakim Borg",
RAVE: A variational autoencoder for fast and high-quality neural audio synthesis,https://arxiv.org/pdf/2111.05011,,,,Preprint,2021,"Deep generative models applied to audio have improved by a large margin the state-of-the-art in many speech and music related tasks. However, as raw waveform modelling remains an inherently difficult task, audio generative models are either computationally intensive, rely on low sampling rates, are complicated to control or restrict the nature of possible signals. Among those models, Variational AutoEncoders (VAE) give control over the generation by exposing latent variables, although they usually suffer from low synthesis quality. In this paper, we introduce a Realtime Audio Variational autoEncoder (RAVE) allowing both fast and high-quality audio waveform synthesis. We introduce a novel two-stage training procedure, namely representation learning and adversarial fine-tuning. We show that using a post-training analysis of the latent space allows a direct control between the reconstruction fidelity and the representation compactness. By leveraging a multi-band decomposition of the raw waveform, we show that our model is the first able to generate 48kHz audio signals, while simultaneously running 20 times faster than real-time on a standard laptop CPU. We evaluate synthesis quality using both quantitative and qualitative subjective experiments and show the superiority of our approach compared to existing models. Finally, we present applications of our model for timbre transfer and signal compression. All of our source code and audio examples are publicly available.","Antoine Caillon, Philippe Esling",
VampNet: Music generation via masked acoustic token modeling,https://arxiv.org/pdf/2307.04686,,,,ISMIR,2023,"We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.","Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo",
M and Jam Factory,https://www.jstor.org/stable/3680237,,,,Computer Music Journal,1987,"Both M and Jam Factory are programs that, through the use of graphic and gestural interfaces, provide an environment for composing and performing with MIDI that is quite analogous to the control panel of an airplane.",David Zicarelli,
The Synthetic Performer in The Context of Live Performance,https://quod.lib.umich.edu/i/icmc/bbp2372.1984.026/1/--synthetic-performer-in-the-context-of-live-performance?page=root;size=150;view=text,,,,ICMC,1984,"We describe a new area of computer-modeled intelligence, aimed at capturing two of today's most common human activities-music perception and rhythmic response. The two are most evident in an ensemble of performing musicians, where each member is both an expert listener and an expert executor of some response-often prescribed by a score, and generally inviting the addition of expressive content.",Barry Vercoe,
Mimi4x: an interactive audio-visual installation for high-level structural improvisation,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5583232,https://www.youtube.com/watch?v=lFlFrHdJwwI,,,ICME,2010,"We present Mimi4x (Multimodal Interaction for Musical Improvisation), an interactive audio-visual installation that engages members of the general public in high-level structural decision-making in musical improvisation. Through a Musical Instrument Digital Interface controller, the user manages four instances of the Mimi 1.5 improvisation system. The visuals in each Mimi system animate the generation of new music, depicting the recombination of musical material and the flow of generated music in time. A composer prepares original music material for each Mimi to use in its improvisation. Through Mimi4x, the user is directly involved in the improvisation/composition process and takes on some of the role of the composer. The user chooses which Mimi instance performs and when, and can adjust the recombination rate for, and playback volume of, each Mimi instance during performance. This simple method of interaction is accessible to all users regardless of their musical background, yet gives the user significant and satisfying influence over the music generated.","Alexandre R.J. François, Isaac Schankler and Elaine Chew",
Approaches to Musical Expression in Harmonix Video Games (The Axe),https://www.worldscientific.com/doi/10.1142/9789813140103_0002?srsltid=AfmBOoqmcD0lfDoNKh40KciyDVgsfr8l_F0klnusJXtpqNt6mJ3aBta2,,,,Mathemusical Conversations: Mathematics and Computation in Music Performance and Composition,2016,"Harmonix Music Systems was founded in 1995 with the mission “to let everyone in the world experience the joy of making music.” Over the past 20 years, Harmonix has released over 20 game titles that, to various degrees of success, have let players feel musical and express themselves musically. This chapter explores five Harmonix games, each of which has taken a different approach to enable player musical expression: The Axe (a joystick-controlled improvisation system), Amplitude (an arcade-style multi-track rhythm game), Karaoke Revolution (a microphone-based singing game), Guitar Hero and Rock Band (multi-instrument rock band simulation game), and Fantasia: Music Evolved (Kinect-based musical exploration game based on the classic Disney movie). It addresses a central dilemma in designing music video games: can players enjoy being expressive while also driving towards the goal of “winning the game?”",Eran Egozy,
Reflections on Eight Years of Instrument Creation with Machine Learning,https://www.nime.org/proceedings/2020/nime2020_paper45.pdf,,,,NIME,1997,"Machine learning (ML) has been used to create mappings for digital musical instruments for over twenty-five years, and numerous ML toolkits have been developed for the NIME community. However, little published work has studied how ML has been used in sustained instrument building and performance practices. This paper examines the experiences of instrument builder and performer Laetitia Sonami, who has been using ML to build and refine her Spring Spyre instrument since 2012. Using Sonami’s current practice as a case study, this paper explores the utility, opportunities, and challenges involved in using ML in practice over many years. This paper also reports the perspective of Rebecca Fiebrink, the creator of the Wekinator ML tool used by Sonami, revealing how her work with Sonami has led to changes to the software and to her teaching. This paper thus contributes a deeper understanding of the value of ML for NIME practitioners, and it can inform design considerations for future ML toolkits as well as NIME pedagogy. Further, it provides new perspectives on familiar NIME conversations about mapping strategies, expressivity, and control, informed by a dedicated practice over many years.","Rebecca Fiebrink, Laetitia Sonami",
AI-terity 2.0: An Autonomous NIME Featuring GANSpaceSynth Deep Learning Model,https://nime.pubpub.org/pub/9zu49nu5/release/1,https://www.nime.org/proc/nime20_65/index.html ,,,NIME,2021,"In this paper we present the recent developments in the AI-terity instrument. AI-terity is a deformable, non-rigid musical instrument that comprises a particular artificial intelligence (AI) method for generating audio samples for real-time audio synthesis. As an improvement, we developed the control interface structure with additional sensor hardware. In addition, we implemented a new hybrid deep learning architecture, GANSpaceSynth, in which we applied the GANSpace method on the GANSynth model. Following the deep learning model improvement, we developed new autonomous features for the instrument that aim at keeping the musician in an active and uncertain state of exploration. Through these new features, the instrument enables more accurate control on GAN latent space. Further, we intend to investigate the current developments through a musical composition that idiomatically reflects the new autonomous features of the AI-terity instrument. We argue that the present technology of AI is suitable for enabling alternative autonomous features in audio domain for the creative practices of musicians.","Koray Tahiroğlu, Miranda Kastemaa, and Oskar Koli",
Composing the Assemblage: Probing Aesthetic and Technical Dimensions of Artistic Creation with Machine Learning (Case 1),https://watermark02.silverchair.com/comj_a_00658.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAzowggM2BgkqhkiG9w0BBwagggMnMIIDIwIBADCCAxwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM7ywKGsiS99fY-K7yAgEQgIIC7ULMaiRpyOyojBfhd_txkYfJ1aBEo3Kw9WWk-YOASJ_PApCtM33ew6LulWGVjf_nj-3WZ4iNbh3AI-fa6IajUUKxH2hCn4bCSo1jhrtuEmthby8K7rm8eX1RoiSZCoCRyxchu-WLIarB8zycoEF_VETiP5Qo55dfWwhQ9YNvNx9rTQflFvljhss4ooWhC9_4Gzgeaex3z3Weos2Vv4OWKtZh2esoAcZallj0C4nYqy0gaEOMIuLMIhYf1SMnrDpfbt-DZNm7Y1D8T8bJIPqf_HR6zlcBTIInJfJ6dof-n7nHFCk80gMeBUgSKrBBXCKDhYN3cqgvgSdAedsKVQTMTW0vUwhHPHKBlihL-hURc3vOOtnH1-6Jlmeh9MZsR3vq7db2UQoAEumdW8FvR1qVJRymbdwNLoq21OA8SLkNN8cCr5X-744_PqUsCsRZwO5XeyUBfQO0yM3blEbcSDYrrIogf1OcfUKGue-9bs9ozsYmuuslSmYv6sS9H4GXdMMqtP9ICQ4us6zudP9UAmPJcG5jccg9mfh5VLKvwcxYp8e5mWI8xwwNb_gcW90xMUGPaizjDfJBg6m2MX2a7tJ32PDYwXoJsOUHUFmXeCX9xA2kQ5dRHW1RTjuhhPhZbn5mJr4KXW3gPhaXcpekUNdMhT84rVrAU7N3gp9_dgEg9qacmtq-BC8bPBZwV21R-UtLennP_q7uE59g_-m9vIRqgWRZ_KOloESnhzflHTNS8oxSGPxG4DGxqUL6KCkJex4GsQ1cWogeJ-n1I-M1qR1Gp0rmyl55fch9pqxCLzSDGrGhC25tg5RbcoNMd_CDCaYdVJYHQmE4hRhIaB0-g5j87G0RMz8xKa82s9F0KBWwiTODl7wnQE8PI59_60Kg9krbD3u_UUReC70e4Fp4NywWTBz0bV84ZsjwObzN76jZsUuajaHbbhftYU351a7mSESjsSqyL8kWclFEYGfb6uD204c9G428J9GThgXW3gD3,,,,Computer Music Journal,2022,"In this article, we address the role of machine learning (ML) in the composition of two new musical works for acoustic instruments and electronics through autoethnographic reflection on the experience. Our study poses the key question of how ML shapes, and is in turn shaped by, the aesthetic commitments characterizing distinctive compositional practices. Further, we ask how artistic research in these practices can be informed by critical themes from humanities scholarship on material engagement and critical data studies. Through these frameworks, we consider in what ways the interaction with ML algorithms as part of the compositional process differs from that with other music technology tools. Rather than focus on narrowly conceived ML algorithms, we take into account the heterogeneous assemblage brought into play: from composers, performers, and listeners to loudspeakers, microphones, and audio descriptors. Our analysis focuses on a deconstructive critique of data as being contingent on the decisions and material conditions involved in the data-creation process. It also explores how interaction among the human and nonhuman collaborators in the ML assemblage has significant similarities to—as well as differences from—existing models of material engagement. Tracking the creative process of composing these works, we uncover the aesthetic implications of the many nonlinear collaborative decisions involved in composing the assemblage.","Artemi-Maria Gioti, Aaron Einbond, Georgina Born",
"The Jam_bot, a real-time system for collaborative free improvisation with music language models",https://ismir2025program.ismir.net/poster_321.html,,,,ISMIR,2025,"In order to design a Generative AI system that could improvise on stage with GRAMMY-winning keyboard virtuoso Jordan Rudess, we developed the “JAM_BOT”, a real- time performance system that could match his eclectic improvisational aesthetics. We debuted the JAM_BOT at a high-stakes sold-out concert to critical acclaim, realizing a series of virtuosic tightly-coupled Human-AI free im- provisations in varying musical styles. Reflecting on our year-long collaboration, we summarize learnings for AI researchers and musicians on the adaptations needed to turn state-of-the-art symbolic music Language Models (LMs) into JAM_BOTS and the engineering required to make them performance-ready. We focus on three aspects: (1) enabling JAM_BOTS to take on different musical roles by adapting music LMs to employ different interaction strategies by modifying the context and conditioning signals; (2) describing how Rudess intentionally structures his improvisation in order to finetune JAM_BOTS to match the style needed for each piece; and (3) showing the optimizations needed to run music LMs in real-time and embed them in a low-latency multi-threaded system that listens, prompts, and schedules model generations seamlessly. We hope these insights enable more musician-AI symbiotic virtuosity.","Lancelot Blanchard, Perry Naseck1, Stephen Brade, Kimaya Lecamwasam, Jordan Rudess, Cheng-Zhi Anna Huang, Joseph Paradiso",
Improvised Performance Following in Real Time for Automatic Accompaniment,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10888267,,,,ICASSP,2025,"Real-time performance tracking is the core component of automatic accompaniment systems. Previous methods typically require full information on the performance score (the human part) and also assume very limited improvisations off the score, otherwise, the tracker will get lost. In this paper, we aim to track a fully improvised performance for automatic accompaniment, in which the only reference is the accompaniment score (the machine part). The key idea is to incorporate semantic-level alignment to model the correspondence between the user performance and the accompaniment. Our system contains two real-time components: The first component is an accompaniment-conditioned quantization model using a Long Short-Term Memory (LSTM) layer. For each performance note onset, we first compute its rough projected score position using previously estimated performance-to-score mapping and then refine the mapping using the quantization results. The second component is a playback control system, which updates the performance-to-score mapping via the performance-quantized time pair (similar to the performance-score alignment pair in traditional automatic accompaniment systems). Experiments show the system achieves better tracking results on expressive piano performance than baselines, allowing score-free tracking on both texture and tempo variations.","Junyan Jiang, Akira Maezawa, Gus Xia",
Real-time Timbre Remapping with Differentiable DSP,https://arxiv.org/pdf/2407.04547,,,,NIME,2024,"Timbre is a primary mode of expression in diverse musical contexts. However, prevalent audio-driven synthesis methods predominantly rely on pitch and loudness envelopes, effectively flattening timbral expression from the input. Our approach draws on the concept of timbre analogies and investigates how timbral expression from an input signal can be mapped onto controls for a synthesizer. Leveraging differentiable digital signal processing, our method facilitates direct optimization of synthesizer parameters through a novel feature difference loss. This loss function, designed to learn relative timbral differences between musical events, prioritizes the subtleties of graded timbre modulations within phrases, allowing for meaningful translations in a timbre space. Using snare drum performances as a case study, where timbral expression is central, we demonstrate real-time timbre remapping from acoustic snare drums to a differentiable synthesizer modeled after the Roland TR-808.","Jordie Shier, Charalampos Saitis, Andrew Robertson, Andrew McPherson",
The reactive accompanist: Adaptation and behavior decomposition in a music system,https://static1.squarespace.com/static/5e13e4b93175437bccfc4545/t/66c21e0aa52ad50cc4cce285/1723997712006/the-reactive-accompanist-adaptation-and-behavior-decomposition-in-a-music-system+.pdf,,,,The Biology and Technology of Intelligent Autonomous Agents,1995,"This paper describes a reactive, behavior-based system that mimics a human musical skill — chord accompaniment of unfamiliar melodies. The system was constructed under the subsumption architecture methodology. This paper discusses the design task of behavior decomposition for such a system, and recommends a strategy of modularizing the minimal adaptive requirements for the desired competence.",Joanna Bryson,
BoB: an interactive improvisational music companion,https://www.cs.cmu.edu/~mmv/papers/thom-agents2k.pdf,https://quod.lib.umich.edu/i/icmc/bbp2372.2001.106/--machine-learning-techniques-for-real-time-improvisational?view=image,,,AGENTS,2000,"This paper introduces a new domain for believable agents (BA) and presents novel methods for dealing with the unique challenges that arise therein. The domain is providing improvisational companionship to a specific musician/user, trading real-time solos with them in the jazz/blues setting. The ways in which this domain both conflicts with and benefits from traditional BA and interactive computer music system approaches are discussed. Band-out-of-the-Box (BOB), an agent built for this domain, is also presented, most novel in that unsupervised machine learning techniques are used to automatically configure BoB's aesthetic musical sense to that of its specific user/musician.",Belinda Thom,
A Bayesian Network for Real-Time Musical Accompaniment,https://proceedings.neurips.cc/paper_files/paper/2001/file/2b0f658cbffd284984fb11d90254081f-Paper.pdf,,,,NIPS,2001,"We describe a computer system that provides a real-time musi(cid:173) cal accompaniment for a live soloist in a piece of non-improvised music for soloist and accompaniment. A Bayesian network is devel(cid:173) oped that represents the joint distribution on the times at which the solo and accompaniment notes are played, relating the two parts through a layer of hidden variables. The network is first con(cid:173) structed using the rhythmic information contained in the musical score. The network is then trained to capture the musical interpre(cid:173) tations of the soloist and accompanist in an off-line rehearsal phase. During live accompaniment the learned distribution of the network is combined with a real-time analysis of the soloist's acoustic sig(cid:173) nal, performed with a hidden Markov model, to generate a musi(cid:173) cally principled accompaniment that respects all available sources of knowledge. A live demonstration will be provided.",Christopher Raphael,
The continuator: Musical interaction with style.,https://web.media.mit.edu/~rebklein/downloads/papers/jnmr%25252E32%25252E3%25252E333%25252E16861.pdf,,,,Journal of New Music Research,2003,"We propose a system, the Continuator, that bridges the gap between two classes of traditionally incompatible musical systems: (1) interactive musical systems, limited in their ability to generate stylistically consistent material, and (2) music imitation systems, which are fundamentally not interactive. Our purpose is to allow musicians to extend their technical ability with stylistically consistent, automatically learnt material. This goal requires the ability for the system to build operational representations of musical styles in a real time context. Our approach is based on a Markov model of musical styles augmented to account for musical issues such as management of rhythm, beat, harmony, and imprecision. The resulting system is able to learn and generate music in any style, either in standalone mode, as continuations of musician’s input, or as interactive improvisation back up. Lastly, the very design of the system makes possible new modes of musical collaborative playing. We describe the architecture, implementation issues and experimentations conducted with the system in several real world contexts.",François Pachet,
ImproteK: Introducing Scenarios into Human-Computer Music Improvisation,https://dl.acm.org/doi/pdf/10.1145/3022635,https://www.youtube.com/watch?v=MsCFoqnvAew,,,CIE,2017,"This article focuses on the introduction of control, authoring, and composition in human-computer music improvisation through the description of a guided music generation model and a reactive architecture, both implemented in the software ImproteK. This interactive music system is used with expert improvisers in work sessions and performances of idiomatic and pulsed music and more broadly in situations of structured or composed improvisation. The article deals with the integration of temporal specifications in the music generation process by means of a fixed or dynamic “scenario” and addresses the issue of the dialectic between reactivity and planning in interactive music improvisation. It covers the different levels involved in machine improvisation: the integration of anticipation relative to a predefined structure in a guided generation process at a symbolic level, an architecture combining this anticipation with reactivity using mixed static/dynamic scheduling techniques, and an audio rendering module performing live re-injection of captured material in synchrony with a non-metronomic beat. Finally, it sketches a framework to compose improvisation sessions at the scenario level, extending the initial musical scope of the system. All of these points are illustrated by videos of performances or work sessions with musicians.","Jérôme Nika, Marc Chemillier, Gérard Assayag",
In a Silent Way: Communication Between AI and Improvising Musicians Beyond Sound,https://arxiv.org/pdf/1902.06442,,,,CHI,2019,"Collaboration is built on trust, and establishing trust with a creative Artificial Intelligence is difficult when the decision process or internal state driving its behaviour isn't exposed. When human musicians improvise together, a number of extra-musical cues are used to augment musical communication and expose mental or emotional states which affect musical decisions and the effectiveness of the collaboration. We developed a collaborative improvising AI drummer that communicates its confidence through an emoticon-based visualisation. The AI was trained on musical performance data, as well as real-time skin conductance, of musicians improvising with professional drummers, exposing both musical and extra-musical cues to inform its generative process. Uni- and bi-directional extra-musical communication with real and false values were tested by experienced improvising musicians. Each condition was evaluated using the FSS-2 questionnaire, as a proxy for musical engagement. The results show a positive correlation between extra-musical communication of machine internal state and human musical engagement.","Jon McCormack, Toby Gifford, Patrick Hutchings, Maria Teresa Llano Rodriguez, Matthew Yee-King, Mark d'Inverno",
RL-Duet: Online Music Accompaniment Generation Using Deep Reinforcement Learning,https://arxiv.org/pdf/2002.03082,,,,AAAI,2020,"This paper presents a deep reinforcement learning algorithm for online accompaniment generation, with potential for real-time interactive human-machine duet improvisation. Different from offline music generation and harmonization, online music accompaniment requires the algorithm to respond to human input and generate the machine counterpart in a sequential order. We cast this as a reinforcement learning problem, where the generation agent learns a policy to generate a musical note (action) based on previously generated context (state). The key of this algorithm is the well-functioning reward model. Instead of defining it using music composition rules, we learn this model from monophonic and polyphonic training data. This model considers the compatibility of the machine-generated note with both the machine-generated context and the human-generated context. Experiments show that this algorithm is able to respond to the human part and generate a melodic, harmonic and diverse machine part. Subjective evaluations on preferences show that the proposed algorithm generates music pieces of higher quality than the baseline method.","Nan Jiang, Sheng Jin, Zhiyao Duan, Changshui Zhang",
SongDriver: Real-time Music Accompaniment Generation without Logical Latency nor Exposure Bias,https://arxiv.org/pdf/2209.06054,,,,MM,2022,"Real-time music accompaniment generation has a wide range of applications in the music industry, such as music education and live performances. However, automatic real-time music accompaniment generation is still understudied and often faces a trade-off between logical latency and exposure bias. In this paper, we propose SongDriver, a real-time music accompaniment generation system without logical latency nor exposure bias. Specifically, SongDriver divides one accompaniment generation task into two phases: 1) The arrangement phase, where a Transformer model first arranges chords for input melodies in real-time, and caches the chords for the next phase instead of playing them out. 2) The prediction phase, where a CRF model generates playable multi-track accompaniments for the coming melodies based on previously cached chords. With this two-phase strategy, SongDriver directly generates the accompaniment for the upcoming melody, achieving zero logical latency. Furthermore, when predicting chords for a timestep, SongDriver refers to the cached chords from the first phase rather than its previous predictions, which avoids the exposure bias problem. Since the input length is often constrained under real-time conditions, another potential problem is the loss of long-term sequential information. To make up for this disadvantage, we extract four musical features from a long-term music piece before the current time step as global information. In the experiment, we train SongDriver on some open-source datasets and an original àiMusic Dataset built from Chinese-style modern pop music sheets. The results show that SongDriver outperforms existing SOTA (state-of-the-art) models on both objective and subjective metrics, meanwhile significantly reducing the physical latency.","Zihao Wang, Kejun Zhang, Yuxing Wang, Chen Zhang, Qihao Liang, Pengfei Yu, Yongsheng Feng, Wenbo Liu, Yikai Wang, Yuntao Bao, Yiheng Yang",
Live Music Models,https://www.arxiv.org/pdf/2508.04651,,,,Preprint,2025,"We introduce a new class of generative models for music called live music models that produce a continuous stream of music in real-time with synchronized user control. We release Magenta RealTime, an open-weights live music model that can be steered using text or audio prompts to control acoustic style. On automatic metrics of music quality, Magenta RealTime outperforms other open-weights music generation models, despite using fewer parameters and offering first-of-its-kind live generation capabilities. We also release Lyria RealTime, an API-based model with extended controls, offering access to our most powerful model with wide prompt coverage. These models demonstrate a new paradigm for AI-assisted music creation that emphasizes human-in-the-loop interaction for live music performance.","Lyria Team: Antoine Caillon, Brian McWilliams, Cassie Tarakajian, Ian Simon, Ilaria Manco, Jesse Engel, Noah Constant, Yunpeng Li, Timo I. Denk, Alberto Lalama, Andrea Agostinelli, Cheng-Zhi Anna Huang, Ethan Manilow, George Brower, Hakan Erdogan, Heidi Lei, Itai Rolnick, Ivan Grishchenko, Manu Orsini, Matej Kastelic, Mauricio Zuluaga, Mauro Verzetti, Michael Dooley, Ondrej Skopek, Rafael Ferrer, Zalán Borsos, Äaron van den Oord, Douglas Eck, Eli Collins, Jason Baldridge, Tom Hume, Chris Donahue, Kehang Han, Adam Roberts",
An On-Line Algorithm for Real-Time Accompaniment,https://www.cs.cmu.edu/~rbd/papers/icmc84accomp.pdf,,,,ICMC,1984,"Real-time accompaniment solves most of the synchronization problems inherent in taped accompaniment; however, this new approach requires the computer to have the ability to follow the soloist. Three subproblems arise: detecting and processing input from the live performer, matching this input against a score of expected input, and generating the timing information necessary to control the generation of the accompaniment. It is expected that the live solo performance will contain mistakes or be imperfectly detected by the computer, so it is necessary to allow for performance mistakes when matching the actual solo against the score. An efficient dynamic programming algorithm for finding the best match between solo performance and the score is presented. In producing the accompaniment, it is necessary to generate a time reference that varies in speed according to the soloist. The notion of virtual time is proposed as a solution to this problem. Finally, experience with two computer systems that produce real-time accompaniment is summarized.",Roger B. Dannenberg,
Active Trading with Impro-Visor,https://musicalmetacreation.org/mume2016/proceedings/Kondak_Impro_Visor.pdf,,,,MUME,2016,"Trading is a common form of jazz improvisation in which one performer exchanges improvisations with others, usually in four- or eight-bar segments. We describe and demonstrate a new feature of Impro-Visor (short for Improvisation Advisor, a program designed to help musicians develop improvisational skills) called active trading, which significantly extends its former automated, but passive, grammarbased trading capabilities. Because Impro-Visor’s active trading can be based on a variety of different response models, it can be viewed as a meta capability, providing for future extensions simply by plugging in code for other trading modules.","Zachary Kondak, Mikayla Konst, Carli Lessard, David Siah, Robert M. Keller",
"The Wekinator: a system for real-time, interactive machine learning in music",https://ismir2010.ismir.net/proceedings/late-breaking-demo-13.pdf,,,,NIME,2009,"We propose a demonstration of The Wekinator, our software system that enables the application of machinelearning based music information retrieval techniques to real-time musical performance, and which emphasizes a richer human-computer interaction in the design of machine learning systems.","Rebecca Fiebrink, Perry R. Cook",
Stacco: Exploring the Embodied Perception of Latent Representations in Neural Synthesis,https://nime.org/proceedings/2024/nime2024_62.pdf,https://www.youtube.com/watch?v=Bt3O-jhSqiU&t=75s ,,,NIME,2024,"The application of neural audio synthesis methods for sound generation has grown significantly in recent years. Among such systems, streaming autoencoders such as RAVE are particularly suitable for instrument design, as they map audio to and from control signals in an abstract latent space with acceptable latency. Despite the uptake of autoencoders in NIME design, little research has been done to characterize the latent spaces of audio models, and to investigate their affordances in practical musical scenarios. In this paper we present Stacco, an instrument specifically designed for the intuitive control of neural audio synthesis latent parameters through the displacement of magnetic objects on a wooden board with four magnetic attractors. We then examine models trained on the same data with different seeds, we explore strategies for more consistent mappings from audio to latent space, and propose a method for stitching the latent space of one model to another. Finally, in a user study, we investigate whether and how these techniques are perceived through embodied practice with Stacco.","Nicola Privato, Victor Shepardson, Giacomo Lepri, and Thor Magnusson",
Gestural hyper instrument collaboration with generative computation for real time creativity,https://dl.acm.org/doi/10.1145/1254960.1254990 ,,,,C&C,2007,"This paper describes the performance, mapping, transformation and representation phases of a model for gesture-triggered musical creativity. These phases are articulated in an example creative environment, Hyper-Shaku (Border-Crossing), an audio-visually augmented shakuhachi performance to demonstrate the adaptive, empathetic response of the generative systems. The shakuhachi is a Japanese traditional end-blown bamboo Zen flute. Its 5 holes and simple construction require subtle and complex gestural movements to produce its diverse range of pitches, vibrato and pitch inflections, making it an ideal candidate for gesture capture. The environment uses computer vision, gesture sensors and computer listening to process and generate electronic music and visualization in real time response to the live performer. The integration of looming auditory motion and Neural Oscillator Network (NOSC) generative modules are implemented in this example.","Kirsty Beilharz, Sam Ferguson",
Gestroviser: Toward Collaborative Agency in Digital Musical Instruments.,https://www.nime.org/proc/wmarley2015/index.html ,,,,NIME,2015,"This paper describes a software extension to the Reactable entitled Gestroviser that was developed to explore musician machine collaboration at the control signal level. The system functions by sampling a performers input, processing or reshaping this sampled input, and then repeatedly replaying it. The degree to which the sampled control signal is processed during replay is adjustable in real-time by the manipulation of a continuous finger slider function. The reshaping algorithm uses stochastic methods commonly used for MIDI note generation from a provided dataset. The reshaped signal therefore varies in an unpredictable manner. In this way the Gestroviser is a device to capture, reshape and replay an instrumental gesture. We describe the result of initial user testing of the system and discuss possible further development.","William Marley, and Nicholas Ward",
The Scale Navigator: A System for Networked Algorithmic Harmony,https://www.nime.org/proc/turczan2019/index.html ,,,,NIME,2019,"The Scale Navigator is a graphical interface implementation of Dmitri Tymoczko's scale network designed to help generate algorithmic harmony and harmonically synchronize performers in a laptop or electro-acoustic orchestra. The user manipulates the Scale Navigator to direct harmony on a chord-to-chord level and on a scale-to-scale level. In a live performance setting, the interface broadcasts control data, MIDI, and real-time notation to an ensemble of live electronic performers, sight-reading improvisers, and musical generative algorithms.","Nathan Turczan, and Ajay Kapur",
Automatic Rhythmic Performance in Max/MSP: the kin.rhythmicator,https://www.nime.org/proc/sioros2011/index.html ,,,,NIME,2011,"We introduce a novel algorithm for automatically generating rhythms in real time in a certain meter. The generated rhythms are ""generic"" in the sense that they are characteristic of each time signature without belonging to a specific musical style. The algorithm is based on a stochastic model in which various aspects and qualities of the generated rhythm can be controlled intuitively and in real time. Such qualities are the density of the generated events per bar, the amount of variation in generation, the amount of syncopation, the metrical strength, and of course the meter itself. The kin.rhythmicator software application was developed to implement this algorithm. During a performance with the kin.rhythmicator the user can control all aspects of the performance through descriptive and intuitive graphic controls.","George Sioros, and Carlos Guedes",
Enabling Multimodal Mobile Interfaces for Musical Performance,https://www.nime.org/proc/roberts2013/index.html ,,,,NIME,2013,"We present research that extends the scope of the mobile application Control, aprototyping environment for defining multimodal interfaces that controlreal-time artistic and musical performances. Control allows users to rapidlycreate interfaces employing a variety of modalities, including: speechrecognition, computer vision, musical feature extraction, touchscreen widgets,and inertial sensor data. Information from these modalities can be transmittedwirelessly to remote applications. Interfaces are declared using JSON and canbe extended with JavaScript to add complex behaviors, including the concurrentfusion of multimodal signals. By simplifying the creation of interfaces viathese simple markup files, Control allows musicians and artists to make novelapplications that use and combine both discrete and continuous data from thewide range of sensors available on commodity mobile devices.","Charles Roberts, Angus Forbes, and Tobias HÃ¶llerer",
A Physical Intelligent Instrument using Recurrent Neural Networks,https://www.nime.org/proc/nss2019/index.html ,,,,NIME,2019,"This paper describes a new intelligent interactive instrument, based on an embedded computing platform, where deep neural networks are applied to interactive music generation. Even though using neural networks for music composition is not uncommon, a lot of these models tend to not support any form of user interaction. We introduce a self-contained intelligent instrument using generative models, with support for real-time interaction where the user can adjust high-level parameters to modify the music generated by the instrument. We describe the technical details of our generative model and discuss the experience of using the system as part of musical performance.","Torgrim Rudland NÃ¦ss, and Charles Patrick Martin",
Mapping to musical actions in the FILTER system,https://www.nime.org/proc/nort2012/index.html ,,,,NIME,2012,"In this paper we discuss aspects of our work in develop-ing performance systems that are geared towards human-machine co-performance with a particular emphasis on improvisation. We present one particular system, FILTER, which was created in the context of a larger project related to artificial intelligence and performance, and has been tested in the context of our electro-acoustic performance trio. We discuss how this timbrally rich and highly non-idiomatic musical context has challenged the design of the system, with particular emphasis on the mapping of machine listening parameters to higher-level behaviors of the system in such a way that spontaneity and creativity are encouraged while maintaining a sense of novel dialogue.","Doug Van Nort, Jonas Braasch, and Pauline Oliveros",
Towards a Human-Centric Design Framework for AI Assisted Music Production,https://www.nime.org/proc/nime20_78/index.html ,,,,NIME,2020,"In this paper, we contribute to the discussion on how to best design human-centric MIR tools for live audio mixing by bridging the gap between research on complex systems, the psychology of automation and the design of tools that support creativity in music production. We present the design of the Channel-AI, an embedded AI system which performs instrument recognition and generates parameter settings suggestions for gain levels, gating, compression and equalization which are specific to the input signal and the instrument type. We discuss what we believe to be the key design principles and perspectives on the making of intelligent tools for creativity and for experts in the loop. We demonstrate how these principles have been applied to inform the design of the interaction between expert live audio mixing engineers with the Channel-AI (i.e. a corpus of AI features embedded in the Midas HD Console. We report the findings from a preliminary evaluation we conducted with three professional mixing engineers and reflect on mixing engineersâ comments about the Channel-AI on social media.","Augoustinos Tsiros, and Alessandro Palladini",
ExSampling: a system for the real-time ensemble performance of field-recorded environmental sounds,https://www.nime.org/proc/nime20_58/index.html ,,,,NIME,2020,"We propose ExSampling: an integrated system of recording application and Deep Learning environment for a real-time music performance of environmental sounds sampled by field recording. Automated sound mapping to Ableton Live tracks by Deep Learning enables field recording to be applied to real-time performance, and create interactions among sound recorder, composers and performers.","Atsuya Kobayashi, Reo Anzai, and Nao Tokui",
Crowd-driven Music: Interactive and Generative Approaches using Machine Vision and Manhattan,https://www.nime.org/proc/nime20_49/index.html ,,,,NIME,2020,"This paper details technologies and artistic approaches to crowd-driven music, discussed in the context of a live public installation in which activity in a public space (a busy railway platform) is used to drive the automated composition and performance of music. The approach presented uses realtime machine vision applied to a live video feed of a scene, from which detected objects and people are fed into Manhattan (Nash, 2014), a digital music notation that integrates sequencing and programming to support the live creation of complex musical works that combine static, algorithmic, and interactive elements. The paper discusses the technical details of the system and artistic development of specific musical works, introducing novel techniques for mapping chaotic systems to musical expression and exploring issues of agency, aesthetic, accessibility and adaptability relating to composing interactive music for crowds and public spaces. In particular, performances as part of an installation for BBC Music Day 2018 are described. The paper subsequently details a practical workshop, delivered digitally, exploring the development of interactive performances in which the audience or general public actively or passively control live generation of a musical piece. Exercises support discussions on technical, aesthetic, and ontological issues arising from the identification and mapping of structure, order, and meaning in non-musical domains to analogous concepts in musical expression. Materials for the workshop are available freely with the Manhattan software.",Chris Nash,
Brainwaves-driven Effects Automation in Musical Performance,https://www.nime.org/proc/nime20_105/index.html ,,,,NIME,2020,"A variety of controllers with multifarious sensors and functions have maximized the real time performers control capabilities. The idea behind this project was to create an interface which enables the interaction between the performers and the effect processor measuring their brain waves amplitudes, e.g., alpha, beta, theta, delta and gamma, not necessarily with the userâs awareness. We achieved this by using an electroencephalography (EEG) sensor for detecting performerâs different emotional states and, based on these, sending midi messages for digital processing units automation. The aim is to create a new generation of digital processor units that could be automatically configured in real-time given the emotions or thoughts of the performer or the audience. By introducing emotional state information in the real time control of several aspects of artistic expression, we highlight the impact of surprise and uniqueness in the artistic performance.","Giorgos Filandrianos, Natalia Kotsani, Edmund G Dervakos, Giorgos Stamou, Vaios Amprazis, and Panagiotis Kiourtzoglou",
Designing Percussive Timbre Remappings: Negotiating Audio Representations and Evolving Parameter Spaces,https://www.nime.org/proc/nime2025_66/index.html ,,,,NIME,2025,"Timbre remapping is an approach to audio-to-synthesizer parameter mapping that aims to transfer timbral expressions from a source instrument onto synthesizer controls. This process is complicated by the ill-defined nature of timbre and the complex relationship between synthesizer parameters and their sonic output. In this work, we focus on real-time timbre remapping with percussion instruments, combining technical development with practice-based methods to address these challenges. As a technical contribution, we introduce a genetic algorithm - applicable to black-box synthesizers including VSTs and modular synthesizers - to generate datasets of synthesizer presets that vary according to target timbres. Additionally, we propose a neural network-based approach to predict control features from short onset windows, enabling low-latency performance and feature-based control. Our technical development is grounded in musical practice, demonstrating how iterative and collaborative processes can yield insights into open-ended challenges in DMI design. Experiments on various audio representations uncover meaningful insights into timbre remapping by coupling data-driven design with practice-based reflection. This work is accompanied by an annotated portfolio, presenting a series of musical performances and experiments with reflections.","Jordie Shier, Rodrigo Constanzo, Charalampos Saitis, Andrew Robertson, and Andrew McPherson",
Live Improvisation with Fine-Tuned Generative AI: A Musical Metacreation Approach,https://www.nime.org/proc/nime2025_54/index.html ,,,,NIME,2025,"This paper presents a pipeline to integrate a fine-tuned open-source text-to-audio latent diffusion model into a workflow with Ableton Live for the improvisation of contemporary electronic music. The system generates audio fragments based on text prompts provided in real time by the performer, enabling dynamic interaction. Guided by Musical Metacreation as a framework, this case study reframes generative AI as a co-creative agent rather than a mere style imitator. By fine-tuning Stable Audio Open on a dataset of the first authorâs compositions and field recordings, this approach demonstrates the ethical and practical benefits of open-source solutions. Beyond showcasing the modelâs creative potential, this study highlights the modelâs significant challenges and the need for democratized tools with real-world applications.","Misagh Azimi, and Mo H. Zareei",
"Evolving the Living Looper: Artistic Research, Online Learning, and Tentacle Pendula",https://www.nime.org/proc/nime2025_36/index.html ,,,,NIME,2025,"The Living Looper is a neural audio synthesis looper system for live input. It combines online learning with pre-trained neural network models to resynthesize incoming audio into ""living loops"" that transform over time. This paper describes new features of the Living Looper and musician perspectives on its use. A new graphical interface facilitates use of the instrument by non-programmers and visualizes each loop to aid performers in tracking which loop is making which sound. We also describe a new living loop algorithm including incremental learning with partial least squares regression. Finally, we report on an artistic project using the Looper and lessons learned, resulting in an increased importance of training data and a developing sense of relationality.","Victor Shepardson, Halla Steinunn StefÃ¡nsdÃ³ttir, and Thor Magnusson",
Repurposing a Rhythm Accompaniment System for Pipe Organ Performance,https://www.nime.org/proc/nime2025_16/index.html ,,,,NIME,2025,"This paper presents an overview of a human-machine collaborative musical performance by RaÃ¼l Refree utilizing multiple MIDI-enabled pipe organs at Palau GÃ¼ell, as part of the Organic concert series. Our earlier collaboration focused on live performances using drum generation systems, where generative models captured rhythmic transient structures while ignoring harmonic information. For the organ performance, we required a system capable of generating harmonic sequences in real-time, conditioned on Refree's performance. Instead of developing a comprehensive state-of-the-art model, we integrated a more traditional generative method to convert our pitch-agnostic rhythmic patterns into harmonic sequences. This paper details the development process, the creative and technical considerations behind the final performance, and a reflection on the efficacy and adaptability of the chosen methodology.","Nicholas Evans, Behzad Haki, and Sergi JordÃ",
TungnaÃ¡: a Hyper-realistic Voice Synthesis Instrument for Real-Time Exploration of Extended Vocal Expressions,https://www.nime.org/proc/nime2024_78/index.html ,,,,NIME,2024,"This demo showcases TungnaÃ¡, a new voice synthesis system and software instrument for real-time musical exploration of ""Deep Voice Synthesis"". The design of TungnaÃ¡ emphasizes real-time interaction and customization, enabling artists to manipulate various aspects of the synthesis process and to explore aesthetic artefacts unique to autoregressive neural synthesis. The synthesis engine achieves real-time streaming generation of paralinguistic and extended forms of vocal expression, while controlling them using symbolic text notations drawn from the entire unicode character set, allowing for the creation of new notation systems. The interface provides visual display and mouse- or OSC-controllable interventions into the machine vocalisations. The demo showcases TungnaÃ¡ on a laptop with headphones and a MIDI controller, allowing participants to explore the instrument via both a textual and physical interface.","Victor Shepardson, Jonathan Reus, and Thor Magnusson",
Improvise+=Chain: Listening to the Ensemble Improvisation of an Autoregressive Generative Model,https://www.nime.org/proc/nime2023_94/index.html ,,,,NIME,2023,"This paper describes Improvise+=Chain, an audio-visual installation artwork of autonomous musical performance using artificial intelligence technology. The work is designed to provide the audience with an experience exploring the differences between human and AI-based virtual musicians. Using a transformer decoder, we developed a four-track (melody, bass, chords and accompaniment, and drums) symbolic music generation model. The model generates each track in real time to create an endless chain of phrases, and 3D visuals and LED lights represent the attention information between four tracks, i.e., four virtual musicians, calculated within the model. This work aims to highlight the differences for viewers to consider between humans and artificial intelligence in music jams by visualizing the only information virtual musicians can communicate with while humans interact in multiple modals during the performance.","Atsuya Kobayashi, Ryo Nishikado, and Nao Tokui",
SnakeSynth: New Interactions for Generative Audio Synthesis,https://www.nime.org/proc/nime2023_90/index.html ,,,,NIME,2023,"I present ""SnakeSynth,"" a web-based lightweight audio synthesizer that combines audio generated by a deep generative model and real-time continuous two-dimensional (2D) input to create and control variable-length generative sounds through 2D interaction gestures. Interaction gestures are touch and mobile-compatible and made with analogies to strummed, bowed, brushed, and plucked musical instrument controls. Point-and-click and drag-and-drop gestures directly control audio playback length and intensity. I show that I can modulate sound length and intensity by interacting with a programmable 2D grid and leveraging the speed and ubiquity of web browser-based audio and hardware acceleration to generate time-varying high-fidelity sounds with real-time interactivity. SnakeSynth adaptively reproduces and interpolates between sounds encountered during model training, notably without long training times, and I briefly discuss possible futures for deep generative models as an interactive paradigm for musical expression.",Eric Easthope,
Rethinking Reflexive Looper for structured pop music,https://www.nime.org/proc/mmarchini2017/index.html ,,,,NIME,2017,"Reflexive Looper (RL) is a live-looping system which allows a solo musician to incarnate the different roles of a whole rhythm section by looping rhythms, chord progressions, bassline and more. The loop pedal, is still the most used device for those types of performances, accounting for many of the cover songs performances on youtube, but not all kinds of song apply. Unlike a common loop pedal, each layer of sound in RL is produced by an intelligent looping-agent which adapts to the musician and respects given constraints, using constrained optimization. In its original form, RL worked well for jazz guitar improvisation but was unsuited to structured music such as pop songs. In order to bring the system on pop stage, we revisited the system interaction, following the guidelines of professional users who tested it extensively. We describe the revisited system which can accommodate both pop and jazz. Thanks to intuitive pedal interaction and structure-constraints, the new RL deals with pop music and has been already used in several in live concert situations.","Marco Marchini, FranÃ§ois Pachet, and BenoÃ®t CarrÃ©",
SoundGrasp : A Gestural Interface for the Performance of Live Music,https://www.nime.org/proc/mitchell2011/index.html ,,,,NIME,2011,"This paper documents the first developmental phase of aninterface that enables the performance of live music usinggestures and body movements. The work included focuseson the first step of this project: the composition and performance of live music using hand gestures captured using asingle data glove. The paper provides a background to thefield, the aim of the project and a technical description ofthe work completed so far. This includes the developmentof a robust posture vocabulary, an artificial neural networkbased posture identification process and a state-based system to map identified postures onto a set of performanceprocesses. The paper is closed with qualitative usage observations and a projection of future plans.","Thomas Mitchell, and Imogen Heap",
An Interactive Musical Prediction System with Mixture Density Recurrent Neural Networks,https://www.nime.org/proc/martin2019/index.html ,,,,NIME,2019,"This paper is about creating digital musical instruments where a predictive neural network model is integrated into the interactive system. Rather than predicting symbolic music (e.g., MIDI notes), we suggest that predicting future control data from the user and precise temporal information can lead to new and interesting interactive possibilities. We propose that a mixture density recurrent neural network (MDRNN) is an appropriate model for this task. The predictions can be used to fill-in control data when the user stops performing, or as a kind of filter on the user's own input. We present an interactive MDRNN prediction server that allows rapid prototyping of new NIMEs featuring predictive musical interaction by recording datasets, training MDRNN models, and experimenting with interaction modes. We illustrate our system with several example NIMEs applying this idea. Our evaluation shows that real-time predictive interaction is viable even on single-board computers and that small models are appropriate for small datasets.","Charles Patrick Martin, and Jim Torresen",
"Integrating HyperInstruments , Musical Robots & Machine Musicianship for North Indian Classical Music",https://www.nime.org/proc/kapur2007/index.html ,,,,NIME,2007,"This paper describes a system enabling a human to perform music with a robot in real-time, in the context of North Indian classical music. We modify a traditional acoustic sitar into a hyperinstrument in order to capture performance gestures for musical analysis. A custom built four-armed robotic Indian drummer was built using a microchip, solenoids, aluminum and folk frame drums. Algorithms written towards ""intelligent"" machine musicianship are described. The final goal of this research is to have a robotic drummer accompany a professional human sitar player live in performance.","Ajay Kapur, Eric Singer, Manjinder S. Benning, George Tzanetakis, and Trimpin Trimpin",
Improvised Duet Interaction: Learning Improvisation Techniques for Automatic Accompaniment,https://www.nime.org/proc/gxia2017/index.html ,,,,NIME,2017,"The interaction between music improvisers is studied in the context of piano duets, where one improviser embellishes a melody, and the other plays a chordal accompaniment with great freedom. We created an automated accompaniment player that learns to play from example performances. Accompaniments are constructed by selecting and concatenating one-measure score units from actual performances. An important innovation is the ability to learn how the improvised accompaniment should respond to variations in the melody performance, using tempo and embellishment complexity as features, resulting in a truly interactive performance within a conventional musical framework. We conducted both objective and subjective evaluations, showing that the learned improviser performs more interactive, musical, and human-like accompaniment compared with the less responsive, rule-based baseline algorithm.","Guangyu Xia, and Roger Dannenberg",
Wearable Interfaces for Cyberphysical Musical Expression,https://www.nime.org/proc/godbehere2008/index.html ,,,,NIME,2008,"We present examples of a wireless sensor network as applied to wearable digital music controllers. Recent advances in wireless Personal Area Networks (PANs) have precipitated the IEEE 802.15.4 standard for low-power, low-cost wireless sensor networks. We have applied this new technology to create a fully wireless, wearable network of accelerometers which are small enough to be hidden under clothing. Various motion analysis and machine learning techniques are applied to the raw accelerometer data in real-time to generate and control music on the fly.","Andrew B. Godbehere, and Nathan J. Ward",
Music for Flesh II: informing interactive music performance with the viscerality of the body system,https://www.nime.org/proc/donnarumma2012/index.html ,,,,NIME,2012,"Performing music with a computer and loudspeakers represents always a challenge. The lack of a traditional instrument requires the performer to study idiomatic strategies by which musicianship becomes apparent. On the other hand, the audience needs to decode those strategies, so to achieve an understanding and appreciation of the music being played. The issue is particularly relevant to the performance of music that results from the mediation between biological signals of the human body and physical performance. The present article tackles this concern by demonstrating a new model of musical performance; what I define biophysical music. This is music generated and played in real time by amplifying and processing the acoustic sound of a performer's muscle contractions. The model relies on an original and open source technology made of custom biosensors and a related software framework. The succesfull application of these tools is discussed in the practical context of a solo piece for sensors, laptop and loudspeakers. Eventually, the compositional strategies that characterize the piece are discussed along with a systematic description of the relevant mapping techniques and their sonic outcome.",Marco Donnarumma,
Musical Exoskeletons : Experiments with a Motion Capture Suit,https://www.nime.org/proc/collins2010a/index.html ,,,,NIME,2010,"Gaining access to a prototype motion capture suit designedby the Animazoo company, the Interactive Systems groupat the University of Sussex have been investigating application areas. This paper describes our initial experimentsin mapping the suit control data to sonic attributes for musical purposes. Given the lab conditions under which weworked, an agile design cycle methodology was employed,with live coding of audio software incorporating fast feedback, and more reflective preparations between sessions, exploiting both individual and pair programming. As the suitprovides up to 66 channels of information, we confront achallenging mapping problem, and techniques are describedfor automatic calibration, and the use of echo state networksfor dimensionality reduction.","Nick Collins, Chris Kiefer, Zeeshan Patoli, and Martin White",
BRAAHMS: A Novel Adaptive Musical Interface Based on Users' Cognitive State,https://www.nime.org/proc/byuksel2015/index.html ,,,,NIME,2015,"We present a novel brain-computer interface (BCI) integrated with a musical instrument that adapts implicitly (with no extra effort from user) to users' changing cognitive state during musical improvisation. Most previous musical BCI systems use either a mapping of brainwaves to create audio signals or use explicit brain signals to control some aspect of the music. Such systems do not take advantage of higher level semantically meaningful brain data which could be used in adaptive systems or without detracting from the attention of the user. We present a new type of real-time BCI that assists users in musical improvisation by adapting to users' measured cognitive workload implicitly. Our system advances the state of the art in this area in three ways: 1) We demonstrate that cognitive workload can be classified in real-time while users play the piano using functional near-infrared spectroscopy. 2) We build a real-time, implicit system using this brain signal that musically adapts to what users are playing. 3) We demonstrate that users prefer this novel musical instrument over other conditions and report that they feel more creative.","Beste Filiz Yuksel, Daniel Afergan, Evan Peck, Garth Griffin, Lane Harrison, Nick Chen, Remco Chang, and Robert Jacob",
When Counterpoint Meets Chinese Folk Melodies,https://papers.nips.cc/paper_files/paper/2020/hash/bae876e53dab654a3d9d9768b1b7b91a-Abstract.html ,,,,NeurIPS,2020,"Counterpoint is an important concept in Western music theory. In the past century, there have been significant interests in incorporating counterpoint into Chinese folk music composition. In this paper, we propose a reinforcement learning-based system, named FolkDuet, towards the online countermelody generation for Chinese folk melodies. With no existing data of Chinese folk duets, FolkDuet employs two reward models based on out-of-domain data, i.e. Bach chorales, and monophonic Chinese folk melodies. An interaction reward model is trained on the duets formed from outer parts of Bach chorales to model counterpoint interaction, while a style reward model is trained on monophonic melodies of Chinese folk songs to model melodic patterns. With both rewards, the generator of FolkDuet is trained to generate countermelodies while maintaining the Chinese folk style. The entire generation process is performed in an online fashion, allowing real-time interactive human-machine duet improvisation. Experiments show that the proposed algorithm achieves better subjective and objective results than the baselines.","Nan Jiang, Sheng Jin, Zhiyao Duan, Changshui Zhang",
Toward Real-Time Recognition of Instrumental Playing Techniques for Mixed Music: A Preliminary Analysis,https://hal.science/hal-04263718/file/Toward_Real_Time_Recognition_of_Instrumental_Playing_Techniques_for_Mixed_Music__A_Preliminary_Analysis_camera-ready.pdf,,,,ICMC,2023,"In contemporary mixed music, real-time digital sound processing is often applied to live instrumental performances. However, switching between sound effects often relies on manual computer operation or the performer’s foot pedal, burdening the operator. This research aims to develop a system that automatically classifies instrument playing techniques and switches sound effects according to the result of the classification, reducing the burden on the operator and expanding creative possibilities in contemporary mixed music. To realize such a system, the classification accuracy of existing research is not sufficient. In this study, we focused on the flute and tested various input data formats to improve the accuracy of the classification. The results show that using a number of frames of around 15 with the Log-Mel-Spectrogram (LMS) data format improves accuracy. Furthermore, we have measured the computational times of some classification algorithms to assure that the system can actually be used in real time. We found that Multi-Layer Perceptron (MLP) with LMS data format was the best choice among them because it had high accuracy and was fast enough, while other algorithms had concerns about computational speed","Nicolas Brochec, Tsubasa Tanaka",
The New StochGran: Expanded Stochastic Granular Synthesis Tools,https://drive.google.com/file/d/1aqgqYWbB1tgwU7ulXZguvKgcCO7VeCM8/view?usp=drive_link,,,,ICMC,2023,"Kieran McAuliffe and Mara Helmuth have updated and expanded previous granular synthesis software instruments in the RTcmix music programming language and created MaxMSP externals for a new interface. The original nonreal-time Cmix instruments by Helmuth allowed composers to create complex, changing timbres by specifying moving stochastic distributions of grains. The new instruments allow for much finer and real-time control of these distributions. This paper also introduces the multi-channel RTcmix instrument which generates stochastic grain clouds in up to 16 channels. In addition, two MaxMSP externals, based on the Cmix instruments, have been programmed for synthesis and sampling, with an expanded graphical user interface replacing the previous StochGran application. These tools provide expanded control options and facilitate more flexible and powerful performances and improvisations. Finally, compositions showcasing the new possibilities of the software are discussed, including McAuliffe's Jealousy…guilt, in eight channels, and Helmuth's Slither, Bursty Splat for laptop ensemble.","Kieran McAuliffe, Mara Helmuth",
LEM: a sound object that performs Live Electronic Music and proposes a new way to compose and distribute music,https://www.fulcrum.org/epubs/8910jx22c?locale=en#page=181,,,,ICMC,2022,"Since the invention of the phonograph the majority of the musical releases are made in such a way that same audio at every playback is executed. With this work I propose a sound object (Live Electronic Music: LEM) that is programmed to interpret the musical piece, always with a different variation at every iteration. LEM is a stand-alone, iterative composition device. A small plastic box houses a single board computer running an algorithm that plays random audio samples selected from a sample bank. Further, the algorithm is programmed to alter the variables of the audio filters randomly and in random instances, within defined limits. This flexibility is an important compositional factor as the decision, with this practice, is not single and concrete (i.e., the composer specifically determines the way for the piece to be played) but probabilistic (i.e., the composer determines the likelihood of more than one way for the piece to be played, as in aleatoric music). This work proposes a new way of music distribution shaping the way a composer makes a decision, and the audience perceives live and prerecorded music. The piece reveals itself endlessly, exposing new dimensions and dynamics drawn from a table of acoustic elements",Spyros Polychronopoulos,
MYtrOmbone: exploring gesture controlled live electronics in solo trombone performance,https://www.open-access.bcu.ac.uk/14157/1/MYtrOmbone-2020-FINAL-1.2.pdf,,,,ICMC,2021,"MYtrOmbone is an interactive system that allows a trombonist to manipulate live electronic audio processing by analysis of the slide position engaged. The system is designed and developed at Integra Lab using an IMU prosthetic (Thalmic Labs Myo armband) worn on the right forearm as a control device. MyoMapper software translates armband data to OSC messages, and a Neural Network machine learning system detects trombone positions from the incoming OSC data. These positions are then mapped to audio signal processing in the Integra Live software that is controlled by the trombonist. This paper presents the musical context that has led to the development of the system, outlining the way that the system works and its application within the first piece composed for it, 146 Lucina. Finally, we suggest further work to develop and refine the system in the future","James Dooley, Simon Hall",
Generate or Search: Na¨ıve Search Can Augment Piano Improvisation Experiences of Novice Players,https://drive.google.com/file/d/11ijp44Zm4TLHWy7l7Bhtu6jiHRWZTF2C/view?usp=drive_link,,,,ICMC,2021,"Deep learning techniques have great potential to augment human skills computationally; for example, it is possible to develop a piano that offers novice players a playing experience of being like a skillful pianist by automatically enhancing their performances. However, we also suspect that the employment of automatic generation based on deep learning does not always lead to the best outcome. In this study, for the purpose of popularizing the enjoyment of improvisation experiences, we compared two piano-based interactive systems that rely on deep-learning-based note generation and search-and-replay approaches respectively. More specifically, the latter system takes a player’s context into account, searching a dataset containing numerous piano performances for a related performance. It then simply replays a performance excerpt for a short time instead of using automatic generation. Interestingly, our user study, conducted at a public music event, suggested that the search-and-replay approach offers players an improvisation experience that is comparable or, in some aspects, better. Our discussion based on the results presents implications for designing interactive systems for computationally augmenting the skills of novice players.",Hiromu Yakura,
Drumductor: A Gesture-Augmented Drum Pattern Generator,https://www.fulcrum.org/epubs/9880vt18d?locale=en#page=412 ,,,,ICMC,2019,"In this paper we introduce the drumductor which combines hand gesture tracking, machine learning, and a drum pattern prototype software to generate and control drum patterns in real-time. Drumductor aims to provide an alternative way to create and interact with synthetically generated drum sequences where predefined drum patterns, fills, and single drum hits can be“conducted” rather than fully performed or fully sequenced. The drumductor system can be utilised in widely-used digital audio workstation (DAW) environments as well as in live performance settings where users are provided with a blended drum pattern generator tool for generating drum patterns","Xianda Wang, Tae Hong Park",
Imitation Game: Real-time Decision-making in an Inter- active Composition for Human and Robotic Percussionist,https://interagency.iem.at/pdfs/Gioti_ICMC2019.pdf,,,,ICMC,2019,"This paper describes an interactive composition for human and robotic percussionist exploring decision-making processes in the context of composed interaction scenarios. The composition is based on a dynamic form, shaped by decisions made by the musician and the robotic percussionist in real-time. Using a Neural Network trained to recognize different instruments and playing techniques, the robotic percussionist makes long-term decisions based on metrics of musical contrast. Similarly, the musician interprets a non-linear score, which enables him/her to interact with the robotic percussionist in realtime. The paper describes various components of the system, including the auditory processing and decisionmaking stage, and introduces a framework for artistic experimentation borrowing evaluation methods from human-computer improvisation.",Artemi - Maria Gioti,
Proposal of Score-Following Reflecting Gaze Information on Cost of DP matching,https://quod.lib.umich.edu/i/icmc/bbp2372.2017.022/--proposal-of-score-following-reflecting-gaze-information?view=image,,,,ICMC,2017,"The goal of our study is to build a score-following system employing keying information and gaze information. A score-following system, which estimates the user’s current performance position on a musical score, is one of the fundamental technologies of real-time computer accompaniment. Conventional score-following systems have estimated the current performance position based only on keying information. However, this is difficult to do with a high degree of accuracy in the case where the player starts playing from a point on the score which is different from the previous performance position, and performs from a score including repeated phrases. In such cases errors and delays occur in score-following. Therefore, the proposed system solves this problem by using not only keying information but also gaze information, which the expresses the player’s thought. This research deals with the technical problems of reducing noise (interference) of gaze and reflecting gaze information. Therefore, noise is reduced by creating a model (gaze model) that predicts gaze movement by HMM. Also, we realize score-following which is not affected by errors, by reflecting the gaze likelihood calculated from the gaze model in the cost of DP matching.","Shiori Terasaki, Yoshinari Takegawa, Keiji Hirata",
InMuSIC: an Interactive Multimodal System for Electroacoustic Improvisation,https://quod.lib.umich.edu/i/icmc/bbp2372.2016.001/1/--inmusic-an-interactive-multimodal-system-for-electroacoustic?page=root;size=150;view=pdf,https://dl.acm.org/doi/10.1145/2757226.2757375 ,,,ICMC,2016,InMuSIC is an Interactive Musical System (IMS) designedfor electroacoustic improvisation (clarinet and live electronics). The system relies on a set of musical interactions based on the multimodal analysis of the instrumentalist’s behaviour: observation of embodied motion qualities (upper-body motion tracking) and sonic parameters(audio features analysis). Expressive cues are computedat various levels of abstraction by comparing the multimodal data. The analysed musical information organisesand shapes the sonic output of the system influencing various decision-making processes. The procedures outlinedfor the real-time organisation of the electroacoustic materials intend to facilitate the shared development of bothlong-term musical structures and immediate sonic interactions. The aim is to investigate compositional and performative strategies for the establishment of a musical collaboration between the improviser and the system.,Giacomo Lepri,
AIIS: An Intelligent Improvisational System,https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://quod.lib.umich.edu/cgi/p/pod/dod-idx/aiis-an-intelligent-improvisational-system.pdf%3Fc%3Dicmc%3Bidno%3Dbbp2372.2016.084%3Bformat%3Dpdf&ved=2ahUKEwiu2Yf8jsiPAxUjIkQIHew9DxEQFnoECBgQAQ&usg=AOvVaw0XVo6rmgcveD3yfnPVmhPy,,,,ICMC,2016,"The modern use of electronic sound in live performance (whether by instrument or composed processing) has continued to give rise to new explorations in its implementation. With the construction of Aiis, we sought to build an interactive performance system which allows for musical improvisation between a live performer and a computer generated sound world based on feedback between these components. Implemented in Pure Data, the system’s micro and macro decisions generate a programmable musical ”personality” derived from probabilistic measures in reaction to audio input. The system’s flexibility allows factors to be modified in order to make wholly new and original musical personalities.","Steven Leffue, Grady Kestler",
"Notes on “Culture of Fire” for Analog Neural Network Synthesizer, Geiger Muller Counters and Computer",https://quod.lib.umich.edu/i/icmc/bbp2372.2015.070/--notes-on-culture-of-fire-for-analog-neural-network?view=image,,,,ICMC,2015,The “Culture of Fire” is an ongoing live performance piece constructed of residue from a project that was started by David Tudor and others to turn INTEL’s now defunct Electronically Trainable Analog Neural Net (ETANN) into a music synthesizer. A secondary layer of “control” in the live performance of the piece is that Geiger Muller Tube triggers are used as the source of actuation and location distribution of sonic events.,Scot Gresham-Lancaster,
“There is pleasure...”: An Improvisation Using the AAIM Performance System,https://quod.lib.umich.edu/cgi/p/pod/dod-idx/there-is-pleasure-an-improvisation-using-the-aaim.pdf?c=icmc;idno=bbp2372.2015.083,,,,ICMC,2015,"This paper will present AAIM, an algorithmic performance system designed to facilitate performance and improvisation of electronic music through the variation of precomposed materials at a micro level. It is the author’s belief that such an approach allows performers to freely focus on larger macro elements of the work such as the form, texture, timbre, and spatialisation, while still incorporating complex variations at the micro level of triggering individual sound events. In addition to presenting the AAIM system, this paper will also discuss “There is pleasure...”, a variable form improvisation in the Electroacoustic idiom performed using the AAIM system. “There is pleasure...” provides a framework within which the performer is free to explore the capabilities of both the AAIM system and the FM synthesis engine used throughout the performance. A further goal of the improvisation is to explore new ways of approaching techniques such as Frequency Modulation and regular pulse within electroacoustic music.",Simon Fay,
Directed Transitional Composition for Gaming and Adaptive Music Using Q-Learning,http://smc.afim-asso.org/smc-icmc-2014/papers/images/VOL_1/0332.pdf,,,,ICMC,2014,"One challenge relating to the creation of adaptive music involves generating transitions between musical ideas. This paper proposes a solution to this problem based on a modification of the Q-Learning framework described by Reese, Yampolskiy and Elmaghraby. The proposed solution represents chords as states in a domain and generates a transition between any two major or minor chords by finding a pathway through the domain in a manner based on a Q-Learning framework. To ensure that the transitional chords conform to the tonalities defined by the start and goal chords, only chords that contain notes that are found in combined pentatonic scales built from the start and goal chords are included within the domain. This restriction increases the speed of pathfinding and improves the conformation of the transitions to desirable tonal spaces (in particular the keys most closely related to the start and goal chords). This framework represents an improvement over previous music generation systems in that it supports transitions from any point in a musical cue to any point in another, and these transitions can be rendered in real time. A general method for implementing this solution in a video game is also discussed.","Jason Cullimore, Howard Hamilton, David Gerhard",
Real-time Breeding Composition System by means of Genetic Programming and Breeding Procedure,http://speech.di.uoa.gr/ICMC-SMC-2014/images/VOL_1/0402.pdf,,,,ICMC,2014,"The use of laptop computers to produce real-time music and multimedia performances has increased significantly in recent years. In this paper, I propose a new method of generating club-style loop music in real time by means of interactive evolutionary computation (IEC). The method includes two features. The first is the concept of “breeding” without any consciousness of generation. The second is a multiple-ontogeny mechanism that generates several phenotypes from one genotype, incorporating ideas of coevolution and multi-objective optimization. The proposed method overcomes certain limitations of IEC, namely the burden of interactive evaluation and the narrow search domain resulting from handling few individuals. A performance system that generates club-style loop music from the photo album in mobile devices is implemented by means of the proposed method. This system is then tested, and the success of performances with the implemented system indicates that the proposed methods work effectively.",Daichi Ando,
Real-time Music Composition through P-timed Petri Nets,https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.icmc14-smc14.net/images/proceedings/OS25-B02-Real-timeMusicComposition.pdf&ved=2ahUKEwiGqf3qk8KPAxVpI0QIHRNFIHEQFnoECBcQAQ&usg=AOvVaw2zK9lLF8_xl7YIHX-Ga66P,,,,ICMC,2014,"This paper introduces a new real-time concept of reconfigurable P-timed Petri nets. Our goal is to provide a formal model to build and modify a net on the fly. In the first part of the article, the original P-timed extensions are summarized. Then we define an endomorphism that alters the original Petri net in real time; for instance one can change the number of tokens or the net structure. The endomorphism is applied to Music Petri nets, showing how this new approach can be effective in real-time synthesis of music. The final case study provides a practical application by illustrating the real-time creation of a simple piano loop.","Adriano Baratè, Goffredo Haus, Luca A. Ludovico",
"Swarm Lake: A Game of Swarm Intelligence, Human Interaction and Collaborative Music Composition",https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://quod.lib.umich.edu/i/icmc/bbp2372.2014.066/2/--swarm-lake-a-game-of-swarm-intelligence-human-interaction%3Fpage%3Droot%3Bsize%3D150%3Bview%3Dtext&ved=2ahUKEwjk_er3x8mPAxVDOUQIHZl1C-4QFnoECBgQAQ&usg=AOvVaw04apDc2ILYEA1G29Fuxk6l,,,,ICMC,2014,"In this work we aim to combine a game platform with the concept of collaborative music synthesis. We use bioinspired intelligence for developing a world - the Lake where multiple tribes of artificial, autonomous agents live within, having survival as their ultimate goal. The tribes exhibit primitive social swarm-based behavior and intelligence, which is used for taking actions that will potentially allow to dominate the game world. Tribes’ populations also demonstrate a number of physical properties that restrict their ability to act illimitably. Multiuser intervention is employed in parallel, affecting the automated decisions and the physical parameters of the tribes, thus infusing the gaming orientation of the application context. Finally, sound synthesis is achieved through a complex mapping scheme established between the events occurring in the Lake and the rhythmic, harmonic and dynamic-range parameters of an advanced, collaborative sound composition engine. This complex mapping scheme allows the production of interesting and complicated sonic patterns that follow the performance evolution in both objective and conceptual levels. The overall synthesis process is controlled by the conductor, a virtual entity that determines the synthesis evolution in a way that is very similar to directing an ensemble performance in real world.","Maximos A. Kaliakatsos-Papakostas, Andreas Floros, Konstantinos Drossos, Konstantinos Koukoudis, Manolis Kyzalas, Achilleas Kalantzis",
Conceptual Blending in Biomusic Composition Space:The “Brainswarm” Paradigm,https://www.icmc14-smc14.net/images/proceedings/OS8-B04-ConceptualBlendinginBiomusicCompositionSpace.pdf,,,,ICMC,2014,"Conceptual blending and biomusic composition spaces are approached in this work, in an effort to identify in them any creative potentialities as new compositional trajectories. The basic ideas and objectives of these two spaces are approached through a paradigm, consisting of a relevant, compositional work of the author, namely“Brainswarm”, which employs real-time acquisition of the body/hands gestural information along with the brain activity of the so-called bio-conductor. The latter acts as a mediator between the real (instrumental ensemble) and the virtual (consisting of swarm ontologies) worlds. The nature of the work allows for exploration and discussion upon specific realization, organization and aesthetic issues, surfacing the main conceptual blending axons involved. The proposed compositional trajectory calls for further understanding of the functional mechanisms of the human body and brain, so to be creatively used in a shared, yet blended, aesthetic expression",Leontios J. Hadjileontiadis,
P300 Harmonies: A Brain-Computer Musical Interface,https://mtg.upf.edu/system/files/publications/ZachariasVamvakousis.pdf,,,,ICMC,2014,"We present P300 harmonies: a P300-based Brain-Computer Musical Interface. Using a commercial low-cost EEG device, the user can voluntarily change the harmony of an arpeggio by focusing and mentally counting the occurrences of each note. The arpeggio consists of 6 notes separated by an interval of 175ms. The notes of the arpeggio are controlled through 6 switches, where each switch has two possible states: up and down. When a switch is in the up-state the note produced by this switch is one tone or semitone -depending on the switch- higher than when in the downstate. By focusing on each of the notes of the arpeggio, the user may change -after 12 repetitions- the state of the corresponding switch. The notes of the arpeggio appear in a random order. The state of each switch is shown on a screen. Each switch flashes when the corresponding note is heard. The user can either focus exclusively on the auditory presentation or make use of the visual presentation as well. The interface was presented in a live performance, where the user was able to successfully change the state of all switches with 100% accuracy. An additional preliminary evaluation was performed with 3 more users, in which the selection accuracy was 83.33%.","Zacharias Vamvakousis, Rafael Ramirez",
SkipStep: A Multi-Paradigm Touch-screen Instrument,https://www.avneeshsarwate.com/static/papers/SkipStep.pdf,,,,ICMC,2014,"SkipStep is a novel touchscreen application that allows users to write, perform, and improvise music on multiple virtual instruments via MIDI. It is a looping-based instrument that incorporates step sequencer and keyboard inspired interfaces with generative music capabilities. Additionally, SkipStep allows for collaborative performance over Wi-Fi, allowing users to either send musical material between separate SkipStep instances or collaboratively edit a single SkipStep file. This paper will provide the motivation behind SkipStep’s design, review similar interfaces, describe SkipStep, present a user evaluation of the interface, and detail future work","Avneesh Sarwate, Jeff Snyder",
"AutoChorusCreator : Four-Part Chorus Generator with Musical Feature Control, Using Search SpacesConstructed from Rules of Music Theory",https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://quod.lib.umich.edu/cgi/p/pod/dod-idx/autochoruscreator-four-part-chorus-generator-with-musical.pdf%3Fc%3Dicmc%3Bidno%3Dbbp2372.2014.157%3Bformat%3Dpdf&ved=2ahUKEwjV_KLQtcWPAxXmLUQIHfPqM5oQFnoECBgQAQ&usg=AOvVaw0dBVr2d4U1-HLYYgckjo57,,,,ICMC,2014,"This paper describes AutoChorusCreator(ACC), a system capable of producing, in real-time, a variety of fourpart harmonies from lead sheet music. Current algorithms for generating four-part harmony have established a high standard in producing results following rules of harmony theories. However, it is still a challenging task to increase variation in the output. Detailed constraints for describing musical variation tend to complicate the rules and methods used to search for a solution. Reducing constraints to gain degrees of freedom in variation often lead to generating outputs which do not follow the rules of harmony theories. Our system ACC is based on a novel approach of generating four-part harmony with variations by incorporating two algorithms, statistical rule application and dynamic programming. This dual implementation enables the system to gain the positive aspects of both algorithms. Evaluations indicate that ACC is capable of generating four-part harmony arrangements of lead-music in realtime. We also confirmed that ACC achieved generating outputs with variations without neglecting to fulfil rules of harmony theories","Benjamin Luke Evans, Satoru Fukayama, Masataka Goto, Nagisa Munekata, Tetsuo Ono",
A Multi-agent Interactive composing system for creating “expressive” accompaniment.,https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://icmc14-smc14.musicportal.gr/images/proceedings/PS1-B09-AMulti-agentInteractive.pdf&ved=2ahUKEwjDhaGI8cmPAxWOJ0QIHTw-GEIQFnoECB4QAQ&usg=AOvVaw290hZ2DVNBKKS245sUy-La,,,,ICMC,2014,"This paper describes the approach and an application that the author has adopted for creating real time performance systems whose musical output is created by the interactions of a human performer and a multi-agent system that acts as an ensemble of software “performers”. The music produced typically consists of several distinct textural layers, where all the sounds produced are transformations of the sound made by the human performer. This type of system can be thought of as an “extended” instrument, where the performer effectively “plays” the ensemble. This approach has been used with notated compositions, improvisation performances and for creating installations. This paper focuses on a composition that utilises a notated score, and is concerned with how the score is interpreted in the context of the musical output of the agent ensemble.This system makes use of two broad categories of agent: performers and controllers. Performer agents transform the live sound in various ways, while controller agents’ work at a higher structural level. They specify goal states and determine which agents are currently heard. Each performer agent has a way of transforming the audio input, and has its own internal strategies for determining what it does. The complexity of the performer agents note choice strategies ranges from simple harmony generators, to algorithmic composition systems.",Michael Spicer,
INTEGRATION OF MACHINE LEARNING ALGORITHMS IN THE COMPUTER-ACOUSTIC COMPOSITION GOLDSTREAM VARIATIONS,https://drive.google.com/file/d/1Oo0s6n8-vqFqVPYfSEoU0eO8ReweS5PJ/view?usp=sharing,,,,ICMC,2013,"This paper presents an implementation of a musical interface that utilizes machine learning (ML) attributes in real-time performance. The objective behind the work is to empower performers with an expanded musical palette. This is achieved by employing a variation on a delay effect implemented with neural networks. In this scenario, the delayed signal is an echo of previously performed motifs based on new inputs categorized by an ART (Adaptive Resonance Theory) neural network. An overview of the piece used to test the musical range of the effect will be given, followed by a description of the development rationale for the project. The paper will conclude with a qualitative evaluation of the usability and responsiveness of the effect, as well as its contribution to the aesthetic quality of the composition.","W. Scott Deal, Javier Sánchez",
CONTROLLING DYNAMIC STOCHASTIC SYNTHESIS WITH AN AUDIO SIGNAL,https://drive.google.com/file/d/1MFi9WCRCB5ADiHjejJhh8yVSWziNNkg6/view?usp=sharing,,,,ICMC,2012,"Dynamic stochastic synthesis is a sound synthesis technique devised by Ianis Xenakis. This technique produces waveforms by interpolating a set of breakpoints. At every repetition of the waveform, the breakpoints change their positions accordingly to random walks. In all the previous implementations of this method, synthesis parameters were either fixed or controllable through a graphical user interface. Our intention was to hide most of the parameters from musicians and let them use an input audio signal to intuitively control the synthesis process. We proposed and implemented a novel solution based on a mapping between audio features of an input signal and the synthesis parameters. The amplitude random walk is limited by the minimum and maximum amplitudes of the input signal and parameterized by the scaled spectral entropy. To define the frequency range of stochastic waveforms, the algorithm uses the fundamental frequency for periodic input signals and the spectral centroid for noisy signals. It is possible to change the interpolation method and the number of breakpoints in a waveform using the graphical user interface. The solution was developed as a real-time system and demonstrated to professional composers. They confirmed that the realization is suitable for practical tasks, especially for live performances.","Gordan Krekovic, Igor Brkic",
CALDER’S VIOLIN: REAL-TIME NOTATION AND PERFORMANCE THROUGH MUSICALLY EXPRESSIVE ALGORITHMS,https://drive.google.com/file/d/1Ae9tHSD1mzDqq1uS5Kii8Er6NSCOvsc7/view?usp=sharing,,,,ICMC,2012,"Notation is a central issue in modern western music. Composers have often sought ways of expanding and refining the functionality of notation and, in doing so, have re-shaped the music that they were originally aiming to describe. Other musical traditions have very different uses for notation; some have no use for it at all; each approach creates contrasting musical experiences. The role that electronics and computers have played in music has also influenced the nature and function of notation. More traditional 'live' notation of note/pitchbased music generated algorithmically has proved particularly problematic: musical notation is itself a very complex subject. Composers and technologists have instead used libraries of images, algorithms for the pregeneration of material or simplified notations that can be used as the basis of more improvisatory performances. This paper presents work involving the live presentation of 'traditionally precise' music notation created through algorithmically generated material. This notation can then be performed by a human musician alongside computer-generated diffused sound or other 'real' musicians. Technologies used include the SuperCollider audio programming environment and the INScore notation project with the Open Sound Control protocol used to communicate between them. As well as providing a fascinating musical experience, the process highlights a number of issues concerning performance practice, instrumental technique, rehearsal, time and timing, as well as the nature of notation itself and its relationship to improvisation",Richard Hoadley,
AN AUTONOMOUS TIMBRE MATCHING IMPROVISER,https://drive.google.com/file/d/1zBCdvUkHuSIR2m0trvHEjiPTs1QDSNfZ/view?usp=sharing,,,,ICMC,2011,"A new autonomous musical agent is described which is intended to improvise alongside a human instrumentalist. The system collects high level, symbolic information about the musical behaviour of the human player such as note sequences and rhythmic patters. It also extracts timbral trajectories in MFCC space. It produces permutations of this information using a sound synthesizer which it automatically programs to achieve tone matching. To match tones in real time a genetic algorithm and a data driven approach have been implemented. The genetic algorithm requires an offline phase to generate a bank of settings for real time tone matching but achieves closer tone matches. The data driven approach organises a large random set of sounds from the synthesizer into hierarchical clusters which can be searched in real time for matches. Evaluation of the system is ongoing.",Matthew John Yee-King,
LIVECELL: REAL-TIME SCORE GENERATION THROUGH INTERACTIVE GENERATIVE COMPOSITION,https://drive.google.com/file/d/17o8vxjjuoRcDu33xLszejTcLvmAdeBt1/view?usp=sharing,,,,ICMC,2011,"This paper discusses Livecell, an interactive generative composition and real-time scoring application for user and string quartet. The paper outlines the authors’ rationale, discusses the use of cellular automata in this context and provides an insight into the application’s structural design. The authors summarise their approach to CA musification and interface design as well as score generation and display. The paper concludes with observations on the musical output and discussion of future developments.","Kingsley Ash, Nikos Stavropoulos",
PAPAGEI: An Extensible Automatic Accompaniment System for Live Instrumental Improvisation,https://clarlow.org/wp-content/uploads/2025/06/papagei-an-extensible-automatic-accompaniment-system-for.pdf,,,,ICMC,2009,"PAPAGEI (Parametric-Algorithmic Patchbay-Accompanist Generating Events by Improvisation) is an automatic accompaniment system based on the algorithmic music composition methods of Clarence Barlow and implemented by Salman Bakht. This paper describes the system design and implementation of PAPAGEI. Next, compositional and performance details are described for two pieces that use the PAPAGEI system: resonancia flautomdtica, by Clarence Barlow and PIANO 04h, by Salman Bakht. Lastly, the capabilities and limitations of PAPAGEI with respect to aesthetic results are described and potential additions to the system and future compositional possibilities are listed.","Salman Bakht, Clarence Barlow",
REINFORCEMENT LEARNING FOR LIVE MUSICAL AGENTS,https://composerprogrammer.com/research/rlforlivemusicalagents.pdf,,,,ICMC,2008,"Current research programmes in computer music may draw from developments in agent technology; music may provide an excellent test case for agent research. This paper describes the challenge of building agents for concert performance which allow close and rewarding interaction with human musicians. This is easier said than done; the fantastic abilities of human musicians in fluidity of action and cultural reference makes for a difficult mandate. The problem can be cast as that of building an autonomous agent for the (unforgiving) realtime musical environment. Live music is a challenging domain to model, with high dimensionality of descriptions and fast learning, responses and effective anticipation required. A novel symbolic interactive music system called Improvagent is presented as a framework for the testing of reinforcement learning over dynamic state-action case libraries, in a context of MIDI piano improvisation. Reinforcement signals are investigated based on the quality of musical prediction, and on the degree of influence in interaction. The former is found to be less effective than baseline methods of assumed stationarity and of simple nearest neighbour case selection. The latter holds more promise; an agent may be able to assess the value of an action in response to an observed state with respect to the potential for stability, or the promotion of change in future states, enabling controlled musical interaction.",Nick Collins,
A REAL-TIME GENETIC ALGORITHM IN HUMANROBOT MUSICAL IMPROVISATION ,https://www.johnrhoads.org/demo/HaileGA.pdf,,,,ICMC,2007,"The paper describes an interactive musical system that utilizes a genetic algorithm in an effort to create inspiring collaborations between human musicians and an improvisatory robotic xylophone player. The robot is designed to respond to human input in an acoustic and visual manner, evolving a human-generated phrase population based on a similarity driven fitness function in real time. The robot listens to MIDI and audio input from human players and generates melodic responses that are informed by the analyzed input as well as by internalized knowledge of contextually relevant material. The paper describes the motivation for the project, the hardware and software design, two performances that were conducted with the system, and a number of directions for future work.","Gil Weinberg, Mark Godfrey, Alex Rae, John Rhoads",
NN MUSIC: IMPROVISING WITH A ‘LIVING’ COMPUTER,https://quod.lib.umich.edu/i/icmc/bbp2372.2007.108/1/--nn-music-improvising-with-a-living-computer?page=root;size=150;view=pdf ,,,,ICMC,2007,"This paper proposes attributes of a living computer music, the product of a live algorithm. It illustrates how these attributes can inform creative design with reference to a real-time system for solo performer-machine collaboration, Neural Network Music, and the PQƒ framework proposed for live algorithms. In NN Music, improvisation is subject to statistical audio analysis. A multilayer perceptron neural network is trained to classify this analysis. The network’s output is mapped in real-time to a stochastic synthesis engine. Mappings are covertly assigned, revisited by both player and machine as a performance develops. As the timing and choice of mapping is unknown, both participants are invited to learn and adapt to a responsive sonic environment that is created afresh on each performance. This offers a novel real-time application of feed-forward neural networks and a challenging, creative technological platform for freely improvised music.",Michael Young,
Sparkler: An Audio-Driven Interactive Live Computer Performance for Symphony Orchestra,https://www.media.mit.edu/publications/sparkler-an-audio-driven-interactive-live-computer-performance-for-symphony-orchestra-2/,,,,ICMC,2002,"This paper describes the design, implementation, and application of an audio-driven computer-based system for live and interactive performance with an orchestra. We begin by describing the compositional and musical goals, emphasizing the electronics aspect. We then discuss the challenges of working with an orchestra and integrating the electronics in a concert hall. We describe the hardware setup and then detail the software implementation, from audio analysis and ""perceptual parameter"" estimation to the generative algorithm.","Tristran Jehan, Tod Machover, Mike Fabio",
Bayesian Real-time Adaptation for Interactive Performance Systems,https://quod.lib.umich.edu/i/icmc/bbp2372.2001.025/1/--bayesian-real-time-adaptation-for-interactive-performance?page=root;size=150;view=pdf ,,,,ICMC,2001,We introduce Bayesian online learning for real time parameter adaptation on a tempo tracking task. We employ a variational extension of the Expectation-Maximization algorithm for online parameter estimation. Simulation results on a real dataset indicate that online adaptation has the potential of capturing performer specific features in real time,"Ali Taylan Cemgil, Bert Kappen",
Harmonizing in Real-Time with Neural Networks,https://quod.lib.umich.edu/i/icmc/bbp2372.2000.170/1/--harmonizing-in-real-time-with-neural-networks?page=root;size=150;view=pdf,,,,ICMC,2000,"The task of real-time harmonization is characterized by the fact that only incomplete knowledge about the input melody is available. In particular, the global structure (phrase segmentation, overall harmonic relationships) and the longer-term melodic and tonal development cannot be taken into account. In this paper we introduce the method of dynamic tonal center forecasting to improve the real-time performance. The main idea of our approach consists in predicting changes of the underlying key, i.e. tonal centers, and in learning harmonic progressions relative to these tonal centers. In this way, both the short-term harmonic prediction performance and the longer-term tonal development can be improved. Our demonstration system HARMOTHEATER offers the possibility to interactively evaluate the proposed real-time harmonization system on arbitrary melodies entered on a MIDI keyboard. The results can be compared to a posteriori harmonizations of the same melodies based on a fundamental key which are carried out by the neural network-based system HARMONET described in [4]. Neural networks for both systems have been trained with four-part chorales of J. S. Bach and Max Reger.","Karin Höthker, Dominik Hörnel",
Combining audio and gestures for a real-time improviser,https://opensoundcontrol.stanford.edu/files/icmc05fin.pdf,,,,ICMC,2005,"combines pitch with information from sensors that capture a flautist's physical gestures. GRI uses neural network based learning to form a characterization of an improviser's behavior during. alearning phase; during performance, it tries to predict the improviser's future behavior","Roberto Morales-Mazanares, Eduardo F. Morales, David Wessel",
SquishySonics: A Deformable Interface for the Physical Control of Real-time AI Sound Generation Tools,https://dl.acm.org/doi/10.1145/3706599.3721175 ,,,,CHI,2025,"SquishySonics is an interface designed to explore mappings between organic physical shapes made through interactions with deformable materials and sound production using machine learning. Deformable materials are proposed as an intuitive yet ambiguous medium for interfacing with AI-driven techniques, such as navigating the latent space in complex AI audio models, which are often difficult for musicians to understand and control expressively.The system comprises three ‘modular’ elements: a camera, an interface-to-sound mapping tool, and sound generation methods. Flexibility and modularity are afforded by loosely coupled components and a ‘bottom-up’ design approach. Polymer clay is used for its ergonomic, low-cost, and familiar properties, enabling hands-on exploration without fear of damaging the interface. Users can represent their perceptual links between shapes and sound by mapping deformations to parameters controlling real-time sound processes. SquishySonics encourages curiousity in the unknown, minimising frustration in complex information tasks, by allowing for physical engagement with abstract concepts.","Trisha Khallaghi, Pete Bennett, Atau Tanaka",
TimToShape: Supporting Practice of Musical Instruments by Visualizing Timbre with 2D Shapes based on Crossmodal Correspondences,https://dl.acm.org/doi/10.1145/3581641.3584053 ,,,,IUI,2023,"Timbre is high-dimensional and sensuous, making it difficult for musical-instrument learners to improve their timbre. Although some systems exist to improve timbre, they require expert labeling for timbre evaluation; however, solely visualizing the results of unsupervised learning lacks the intuitiveness of feedback because human perception is not considered. Therefore, we employ crossmodal correspondences for intuitive visualization of the timbre. We designed TimToShape, a system that visualizes timbre with 2D shapes based on the user’s input of timbre–shape correspondences. TimToShape generates a shape morphed by linear interpolation according to the timbre’s position in the latent space, which is obtained by unsupervised learning with a variational autoencoder (VAE). We confirmed that people perceived shapes generated by TimToShape to correspond more to timbre than randomly generated shapes. Furthermore, a user study of six violin players revealed that TimToShape was well-received in terms of visual clarity and interpretability.","Kota Arai, Yutaro Hirao, Takuji Narumi, Tomohiko Nakamura, Shinnosuke Takamichi, Shigeo Yoshida",
Church Belles: An Interactive System and Composition Using Real-World Metaphors,https://www.nime.org/proc/waite2016/index.html ,,,,NIME,2016,"This paper presents a brief review of current literature detailing some of the issues and trends in composition and performance with interactive music systems. Of particular interest is how musicians interact with a separate machine entity that exercises agency over the creative process. The use of real-world metaphors as a strategy for increasing audience engagement is also discussed. The composition and system Church Belles is presented, analyzed and evaluated in terms of its architecture, how it relates to existing studies of musician-machine creative interaction and how the use of a real-world metaphor can promote audience perceptions of liveness. This develops previous NIME work by offering a detailed case study of the development process of both a system and a piece for popular, non-improvisational vocal/guitar music.",Si Waite,
Non-intrusive Counter-actions: Maintaining Progressively Engaging Interactions for Music Performance,https://www.nime.org/proc/tahironicode287lu2016/index.html ,,,,NIME,2016,"In this paper we present the new development of a semi-autonomous response module for the NOISA system. NOISA is an interactive music system that predicts performer's engagement levels, learns from the performer, decides what to do and does it at the right moment. As an improvement for the above, we implemented real-time adaptive features that respond to a detailed monitoring of the performer's engagement and to overall sonic space, while evaluating the impact of its actions. Through these new features, the response module produces meaningful and non-intrusive counter actions, attempting to deepen and maintain the performer's engagement in musical interaction. In a formative study we compared our designed response module against a random control system of events, in which the former performed consistently better than the latter.","Koray Tahiroglu, Juan Carlos Vasquez, and Johan Kildal",
Unsupervised Play: Machine Learning Toolkit for Max,https://www.nime.org/proc/smith2012a/index.html ,,,,NIME,2012,"Machine learning models are useful and attractive tools for the interactive computer musician, enabling a breadth of interfaces and instruments. With current consumer hardware it becomes possible to run advanced machine learning algorithms in demanding performance situations, yet expertise remains a prominent entry barrier for most would-be users. Currently available implementations predominantly employ supervised machine learning techniques, while the adaptive, self-organizing capabilities of unsupervised models are not generally available. We present a free, new toolbox of unsupervised machine learning algorithms implemented in Max 5 to support real-time interactive music and video, aimed at the non-expert computer artist.","Benjamin D. Smith, and Guy E. Garnett",
Algorithmic Power Ballads,https://www.nime.org/proc/nime21_36/index.html ,,,,NIME,2021,"Algorithmic Power Ballads is a performance for Saxophone and autonomous improvisor, with an optional third performer who can use the web interface to hand-write note sequences, and adjust synthesis parameters. The performance system explores shifting power dynamics between acoustic, algorithmic and autonomous performers through modifying the amount of control and agency they have over the sound over the duration of the performance. A higher-level algorithm how strongly the machine listening algorithms, which analyse the saxophone input, influence the rhythmic and melodic patterns generated by the system. The autonomous improvisor is trained on power ballad melodies prior to the performance and in lieu of influence from the saxophonist and live coder strays towards melodic phrases from this musical style. The piece is written in javascript and WebAudio API and uses MMLL a browser-based machine listening library.",Shelly Knotts,
Support System for Improvisational Ensemble Based on Long Short-Term Memory Using Smartphone Sensor,https://www.nime.org/proc/nime20_77/index.html ,,,,NIME,2020,"Our goal is to develop an improvisational ensemble support system for music beginners who do not have knowledge of chord progressions and do not have enough experience of playing an instrument. We hypothesized that a music beginner cannot determine tonal pitches of melody over a particular chord but can use body movements to specify the pitch contour (i.e., melodic outline) and the attack timings (i.e., rhythm). We aim to realize a performance interface for supporting expressing intuitive pitch contour and attack timings using body motion and outputting harmonious pitches over the chord progression of the background music. Since the intended users of this system are not limited to people with music experience, we plan to develop a system that uses Android smartphones, which many people have. Our system consists of three modules: a module for specifying attack timing using smartphone sensors, module for estimating the vertical movement of the smartphone using smartphone sensors, and module for estimating the sound height using smartphone vertical movement and background chord progression. Each estimation module is developed using long short-term memory (LSTM), which is often used to estimate time series data. We conduct evaluation experiments for each module. As a result, the attack timing estimation had zero misjudgments, and the mean error time of the estimated attack timing was smaller than the sensor-acquisition interval. The accuracy of the vertical motion estimation was 64%, and that of the pitch estimation was 7.6%. The results indicate that the attack timing is accurate enough, but the vertical motion estimation and the pitch estimation need to be improved for actual use.","Haruya Takase, and Shun Shiramatsu",
Esteso: Interactive AI Music Duet Based on Player-Idiosyncratic Extended Double Bass Techniques,https://www.nime.org/proc/nime2024_72/index.html ,,,,NIME,2024,"Extended playing techniques are a crucial characteristic of contemporary double bass practice. Players find their voice by developing a personal vocabulary of techniques through practice and experimentation. These player-idiosyncratic techniques are used in composition, performance, and improvisation. Today's AI methods offer the opportunity to recognize such techniques and repurpose them in real-time, leading to new forms of interactions between musicians and machines. This paper is the result of a collaboration between a composer/double-bass player and researchers, born from the musician's desire for an interactive improvisational experience with AI centered around the practice of his extended techniques. With this aim, we developed Esteso: an interactive improvisational system based on extended technique recognition, live electronics, and a timbre-transfer double-bass model. We evaluated our system with the musician with three duet improvisational sessions, each using different mapping strategies between the techniques and the sound of the virtual double bass counterpart. We collected qualitative data from the musician to gather insights about the three configurations and the corresponding improvisational duets, as well as investigate the resulting interactions. We provide a discussion about the outcomes of our analysis and draw more general design considerations.","Domenico Stefani, Matteo Tomasetti, Filiippo Angeloni, and Luca Turchet",
Improvasher: A Real-Time Mashup System for Live Musical Input,https://www.nime.org/proc/mdavies2014/index.html ,,,,NIME,2014,"In this paper we present Improvasher a real-time musical accompaniment system which creates an automatic mashup to accompany live musical input. Improvasher is built around two music processing modules, the first, a performance following technique, makes beat-synchronous predictions of chroma features from a live musical input. The second, a music mashup system, determines the compatibility between beat-synchronous chromagrams from different pieces of music. Through the combination of these two techniques, a real-time time predict mashup can be generated towards a new form of automatic accompaniment for interactive musical performance.","Matthew Davies, Adam Stark, Fabien Gouyon, and Masataka Goto",
Shaping and Exploring Interactive Motion-Sound Mappings Using Online Clustering Techniques,https://www.nime.org/proc/hscurto2017/index.html ,,,,NIME,2017,"Machine learning tools for designing motion-sound relationships often rely on a two-phase iterative process, where users must alternate between designing gestures and performing mappings. We present a first prototype of a user adaptable tool that aims at merging these design and performance steps into one fully interactive experience. It is based on an online learning implementation of a Gaussian Mixture Model supporting real-time adaptation to user movement and generation of sound parameters. To allow both fine-tune modification tasks and open-ended improvisational practices, we designed two interaction modes that either let users shape, or guide interactive motion-sound mappings. Considering an improvisational use case, we propose two example musical applications to illustrate how our tool might support various forms of corporeal engagement with sound, and inspire further perspectives for machine learning-mediated embodied musical expression.","Hugo Scurto, FrÃ©dÃ©ric Bevilacqua, and Jules FranÃ§oise",
Musicianship for Robots with Style,https://www.nime.org/proc/gimenes2007/index.html ,,,,NIME,2007,"In this paper we introduce a System conceived to serve as the ""musical brain"" of autonomous musical robots or agent-based software simulations of robotic systems. Our research goal is to provide robots with the ability to integrate with the musical culture of their surroundings. In a multi-agent configuration, the System can simulate an environment in which autonomous agents interact with each other as well as with external agents (e.g., robots, human beings or other systems). The main outcome of these interactions is the transformation and development of their musical styles as well as the musical style of the environment in which they live.","Marcelo Gimenes, Eduardo Miranda, and Chris Johnson",
Cognitive Architecture in Mobile Music Interactions,https://www.nime.org/proc/derbinsky2011/index.html ,,,,NIME,2011,"This paper explores how a general cognitive architecture canpragmatically facilitate the development and exploration ofinteractive music interfaces on a mobile platform. To thisend we integrated the Soar cognitive architecture into themobile music meta-environment urMus. We develop anddemonstrate four artificial agents which use diverse learningmechanisms within two mobile music interfaces. We alsoinclude details of the computational performance of theseagents, evincing that the architecture can support real-timeinteractivity on modern commodity hardware.","Nate Derbinsky, and Georg Essl",
Interacting with Musebots,https://www.nime.org/proc/brown2018/index.html ,,,,NIME,2018,"Musebots are autonomous musical agents that interact with other musebots to produce music. Inaugurated in 2015, musebots are now an established practice in the field of musical metacreation, which aims to automate aspects of creative practice. Originally musebot development focused on software-only ensembles of musical agents, coded by a community of developers. More recent experiments have explored humans interfacing with musebot ensembles in various ways: including through electronic interfaces in which parametric control of high-level musebot parameters are used; message-based interfaces which allow human users to communicate with musebots in their own language; and interfaces through which musebots have jammed with human musicians. Here we report on the recent developments of human interaction with musebot ensembles and reflect on some of the implications of these developments for the design of metacreative music systems.","Andrew R. Brown, Matthew Horrigan, Arne Eigenfeldt, Toby Gifford, Daniel Field, and Jon McCormack",
Toward an Emotionally Intelligent Piano: Real-Time Emotion Detection and Performer Feedback via Kinesthetic Sensing in Piano Performance,https://www.nime.org/proc/benasher2013/index.html ,,,,NIME,2013,"A system is presented for detecting common gestures, musical intentions andemotions of pianists in real-time using only kinesthetic data retrieved bywireless motion sensors. The algorithm can detect common Western musicalstructures such as chords, arpeggios, scales, and trills as well as musicallyintended emotions: cheerful, mournful, vigorous, dreamy, lyrical, and humorouscompletely and solely based on low-sample-rate motion sensor data. Thealgorithm can be trained per performer in real-time or can work based onprevious training sets. The system maps the emotions to a color set andpresents them as a flowing emotional spectrum on the background of a pianoroll. This acts as a feedback mechanism for emotional expression as well as aninteractive display of the music. The system was trained and tested on a numberof pianists and it classified structures and emotions with promising results ofup to 92% accuracy.","Matan Ben-Asher, and Colby Leider",
In A State: Live Emotion Detection and Visualisation for Music Performance,https://www.nime.org/proc/avantklooster2014/index.html ,,,,NIME,2014,Emotion is a complex topic much studied in music and arguably equally central to the visual arts where this is usually referred to with the overarching label of aesthetics. This paper explores how music and the arts have incorporated the study of emotion. We then introduce the development of a live audio visual interface entitled In A State that detects emotion from live audio (in this case a piano performance) and generates visuals and electro acoustic music in response.,"Adinda van 't Klooster, and Nick Collins",
The Concatenator: A Bayesian Approach to Real Time Concatenative Musaicing,https://doi.org/10.5281/zenodo.14877471 ,,,,ISMIR,2024,"We present ""The Concatenator,"" a real time system for audio-guided concatenative synthesis. Similarly to Driedger et al.'s ""musaicing"" (or ""audio mosaicing"") technique, we concatenate a set number of windows within a corpus of audio to re-create the harmonic and percussive aspects of a target audio stream. Unlike Driedger's NMF-based technique, however, we instead use an explicitly Bayesian point of view, where corpus window indices are hidden states and the target audio stream is an observation. We use a particle filter to infer the best hidden corpus states in real-time. Our transition model includes a tunable parameter to control the time-continuity of corpus grains, and our observation model allows users to prioritize how quickly windows change to match the target. Because the computational complexity of the system is independent of the corpus size, our system scales to corpora that are hours long, which is an important feature in the age of vast audio data collections. Within The Concatenator module itself, composers can vary grain length, fit to target, and pitch shift in real time while reacting to the sounds they hear, enabling them to rapidly iterate ideas. To conclude our work, we evaluate our system with extensive quantitative tests of the effects of parameters, as well as a qualitative evaluation with artistic insights. Based on the quality of the results, we believe the real-time capability unlocks new avenues for musical expression and control, suitable for live performance and modular synthesis integration, which furthermore represents an essential breakthrough in concatenative synthesis technology.","Christopher J. Tralie, Ben Cantil",
The Robo-Cajon: An Example of Live Performance with Musical Robotics,https://austinfranklinmusic.com/wp-content/uploads/2024/07/The_Robo_Cajon__An_Example_of_Live_Performance_with_Musical_Robotics.pdf,,,,ICMC,2024,"The Robo-Cajon is a robotic musical performer capable of real-time improvisation and live performance with a human performer. It is a wooden cajon, or box-shaped percussion instrument played with the hands, mounted with two push/pull solenoids that receives input from a separate cajon mounted with piezo contact microphones performed by a human. Rhythmic data from the human performer is first classified using a Multi-Layer Perceptron (MLP) and a Hidden Markov Model (HMM) is used for prediction and generating rhythmic patterns based on this prior classification. The Robo-Cajon demonstrates a novel and lightweight approach to human-computer interaction and real-time applications for various machine learning algorithms. The Robo-Cajon was realized using Max and Arduino.",Austin Franklin,
Soundwriter: Real-Time Music Generation for Oral Storytelling through Emotion Mapping,https://www.fulcrum.org/epubs/9880vt18d?locale=en#page=107,,,,ICMC,2019,"Soundwriter is a real-time emotion detection system implemented in MATLAB that takes a spoken story and analyzes its emotional dimensions via two primary processes: 1) semantic analysis and 2) prosodic analysis. The former process classifies emotions by labeling either an incoming stream of detected words or previously typewritten sentences on three emotional dimensions—arousal, valence, and dominance—while the latter process gathers emotional qualities by analyzing low-level audio features of a speaker’s voice, such as global F0 (pitch contour), speech rate, and intensity (loudness). In this paper, we demonstrate a creative application using Max/MSP, where Soundwriter dictates the behavior of the music generation algorithm built in Max/MSP for live performance applications. In particular, we target children as the primary audience and explore ways in which Soundwriter can reinforce the traditional experience of oral storytelling through the lens of child development. Simultaneously, we introduce an interactive scoring system that stimulates a new way of composing music and enables composers to transform poems into interactive music notations","Toshihisa Tsuruoka, Ashley Erika Muniz",
Populous Oscillation: Variety in Interactive Evolutionary Computation for Music Improvisation,https://researchprofiles.ku.dk/en/publications/populous-oscillation-variety-in-interactive-evolutionary-computat,,,,ICMC,2018,"This paper proposes an experimental approach to interactive evolutionary computation-assisted musical improvisation through practice-based research and aesthetics discourse on evolutionary art. The harmonic series of a snare drum acts as source and fitness function for a real-time performance environment created in Max/MSP. The processing is based on the partials of the acoustic drum that, by way of genetic algorithms, populate, evolve and control sound synthesis timbre, tonality and rhythmical patterns. This presents a dynamic human-computer performance augmenting the acoustic instrument or as a mutating artificial agent. The paper seeks to map novel approaches to generative electro-acoustic music by suggesting a rapid variety and flow of fitness in both genetic algorithms and neural networks resembling modes of listening in free improvisation. The results span from prototype recordings, discussions of aesthetics and perspectives on further development, all anchored in the author’s artistic practice",Anders Bach Pedersen,
A Real-Time Music Emotion Modulation System for Soundtracks,https://www.researchgate.net/profile/Bing_Yen_Chang/publication/339676394_A_Real-Time_Music_Emotion_Modulation_System_for_Soundtracks/links/5e5f63964585152ce80504bf/A-Real-Time-Music-Emotion-Modulation-System-for-Soundtracks.pdf,,,,ICMC,2017,"Many entertainment media contain thematic soundtracks, oftentimes achieved by repeatedly reprising a core music piece via modulation of musical parameters, such as Tempo, Dynamics, Pitch, and Mode, to fit the emotional characteristics of the varying scenes. Moreover, interactive media soundtracks need to be responsive to user interaction. This paper implements a real-time MIDI system that automates this emotion modulation process. The system takes as input Valence-Arousal levels to perform real-time musical parameter modulation of the user-specified music piece. Arousal is probabilistically mapped to a weighted sum of Tempo and Dynamics. Valence is mapped with Positive as Major and Negative as Minor, utilizing Relative, Natural, and Harmonic Minor, together with the Pitch register for further shading. Each input event also triggers a MIDI instrumentation change to enhance user engagement. Modulation of Canon in D towards the Happy, Calm, Sad, and Angry quadrants evoked joyous, meditative, depressive, and vengeful emotional characteristics respectively. Real-time modulation, particularly with a fast stream of quadrant-changing inputs, provides a varied and interesting musical experience akin to an emotionally moody music box. This system provides a framework for audio engineers and soundtrack designers to automate production of thematic and adaptive soundtracks for films, video games, and virtual reality.","Bing Yen Chang, Andrew Horner",
"AI See, You See: Human-AI Musical Collaboration in Augmented Reality",https://dl.acm.org/doi/10.1145/3706599.3720052 ,,,,CHI,2025,"The integration of artificial intelligence (AI) into creative practices has opened new possibilities for dynamic human-AI collaboration. While prior research has explored structured, process-based co-creative tasks, real-time, open-ended creative contexts such as musical performance remain underexplored. These environments demand tools that facilitate dynamic interaction, mutual understanding, and the explainability of AI contributions. This work introduces a novel augmented reality (AR) co-creative musical system that visualises AI’s musical actions, such as MIDI outputs and control parameters, through real-time AR visualisations. By embedding these visualisations within the performance space, the system facilitates shared creative processes and elevates AI’s presence. Using an auto-ethnographic approach, we evaluated the system in three musical performances across varied configurations, revealing insights into the role of AR in shaping human-AI collaboration and co-creative dynamics. This work offers design and evaluation insights for AR-enabled co-creative systems.","Yichen Wang, Charles Patrick Martin",
Live Coding with the Cloud and a Virtual Agent,https://nime.pubpub.org/pub/zpdgg2fg/release/1 ,,,,NIME,2021,"The use of crowdsourced sounds in live coding can be seen as an example of asynchronous collaboration. It is not uncommon for crowdsourced databases to return unexpected results to the queries submitted by a user. In such a situation, a live coder is likely to require some degree of additional filtering to adapt the results to her/his musical intentions. We refer to this context-dependent decisions as situated musical actions. Here, we present directions for designing a customisable virtual companion to help live coders in their practice. In particular, we introduce a machine learning (ML) model that, based on a set of examples provided by the live coder, filters the crowdsourced sounds retrieved from the Freesound online database at performance time. We evaluated a first illustrative model using objective and subjective measures. We tested a more generic live coding framework in two performances and two workshops, where several ML models have been trained and used. We discuss the promising results for ML in education, live coding practices and the design of future NIMEs.","Anna Xambó, Gerard Roma, Sam Roig, and Eduard Solaz",
Generative Improv . & Interactive Music Project (GIIMP),https://www.nime.org/proc/whalley2010/index.html ,,,,NIME,2010,"GIIMP addresses the criticism that in many interactive music systems the machine simply reacts. Interaction is addressed by extending Winkler's [18] model toward adapting Paine's [10] conversational model of interaction. Realized using commercial tools, GIIMP implements a machine/human generative improvisation system using human gesture input, machine gesture capture, and a gesture mutation module in conjunction with a flocking patch, mapped through microtonal/spectral techniques to sound. The intention is to meld some established and current practices, and combine aspects of symbolic and sub-symbolic approaches, toward musical outcomes.",Ian Whalley,
"PESI Extended System: In Space, On Body, with 3 Musicians",https://www.nime.org/proc/tahiroglu2013/index.html ,,,,NIME,2013,"This paper introduces a novel collaborative environment (PESI) in whichperformers are not only free to move and interact with each other but wheretheir social interactions contribute to the sonic outcome. PESI system isdesigned for co-located collaboration and provides embodied and spatialopportunities for musical exploration. To evaluate PESI with skilled musicians,a user-test jam session was conducted. Musicians' comments indicate that thesystem facilitates group interaction finely to bring up further intentions tomusical ideas. Results from our user-test jam session indicate that, through some modificationof the 'in-space' response to the improvisation, and through more intuitiveinteractions with the 'on-body' mobile instruments, we could make thecollaborative music activity a more engaging and active experience. Despitebeing only user-tested once with musicians, the group interview has raisedfruitful discussions on the precise details of the system components.Furthermore, the paradigms of musical interaction and social actions in groupactivities need to be questioned when we seek design requirements for such acollaborative environment. We introduced a system that we believe can open upnew ways of musical exploration in group music activity with a number ofmusicians. The system brings up the affordances of accessible technologieswhile creating opportunities for novel design applications to be explored. Ourresearch proposes further development of the system, focusing on movementbehavior in long-term interaction between performers. We plan to implement thisversion and evaluate design and implementation with distinct skilled musicians.","Koray TahiroÄlu, Nuno N. Correia, and Miguel Espada",
An Intelligent Drum Machine for Electronic Dance Music Production and Performance,https://www.nime.org/proc/rvogl2017/index.html ,,,,NIME,2017,"An important part of electronic dance music (EDM) is the so-called beat. It is defined by the drum track of the piece and is a style defining element. While producing EDM, creating the drum track tends to be delicate, yet labor intensive work. In this work we present a touch-interface-based prototype with the goal to simplify this task. The prototype aims at supporting musicians to create rhythmic patterns in the context of EDM production and live performances. Starting with a seed pattern which is provided by the user, a list of variations with varying degree of deviation from the seed pattern is generated. The interface provides simple ways to enter, edit, visualize and browse through the patterns. Variations are generated by means of an artificial neural network which is trained on a database of drum rhythm patterns extracted from a commercial drum loop library. To evaluate the user interface and pattern generation quality a user study with experts in EDM production was conducted. It was found that participants responded positively to the user interface and the quality of the generated patterns. Furthermore, the experts consider the prototype helpful for both studio production situations and live performances.","Richard Vogl, and Peter Knees",
Glitch Delighter : Lighter's Flame Base Hyper-Instrument for Glitch Music in Burning The Sound Performance,https://www.nime.org/proc/quintas2010/index.html ,,,,NIME,2010,"Glitch DeLighter is a HyperInstrument conceived for Glitch music, based on the idea of using fire expressiveness to digitally distort sound, pushing the body and primitive ritualism into a computer mediated sound performance. Glitch DeLighter uses ordinary lighters as physical controllers that can be played by creating a flame and moving it in the air. Droned sounds are played by sustaining the flame and beats by generating sparks and fast flames. The pitch of every sound can be changed moving the flame vertically in the air. This is achieved by using a custom computer vision system as an interface which maps the real-time the data extracted from the flame and transmits those parameters to the sound generator. As a result, the flame visual dynamics are deeply connected to the aural perception of the sound - âthe sound seems to be burningâ. This process establishes a metaphor dramaturgically engaging for an audience. This paper contextualizes the glitch music aesthetics, prior research, the design and development of the instrument and reports on Burning The Soundâ the first music composition created and performed with the instrument (by the author).",Rudolfo Quintas,
CAVI: A Coadaptive Audiovisual InstrumentâComposition,https://www.nime.org/proc/nime22_25/index.html ,,,,NIME,2022,"This paper describes the development of CAVI, a coadaptive audiovisual instrument for collaborative humanmachine improvisation. We created this agent-based live processing system to explore how a machine can interact musically based on a human performerâs bodily actions. CAVI utilized a generative deep learning model that monitored muscle and motion data streamed from a Myo armband worn on the performerâs forearm. The generated control signals automated layered time-based effects modules and animated a virtual body representing the artificial agent. In the final performance, two expert musicians (a guitarist and a drummer) performed with CAVI. We discuss the outcome of our artistic exploration, present the scientific methods it was based on, and reflect on developing an interactive system that is as much an audiovisual composition as an interactive musical instrument.","Cagri Erdem, Benedikte Wallace, and Alexander Refsum Jensenius",
RAW: Exploring Control Structures for Muscle-based Interaction in Collective Improvisation,https://www.nime.org/proc/nime20_91/index.html ,,,,NIME,2020,"This paper describes the ongoing process of developing RAW, a collaborative bodyâmachine instrument that relies on 'sculpting' the sonification of raw EMG signals. The instrument is built around two Myo armbands located on the forearms of the performer. These are used to investigate muscle contraction, which is again used as the basis for the sonic interaction design. Using a practice-based approach, the aim is to explore the musical aesthetics of naturally occurring bioelectric signals. We are particularly interested in exploring the differences between processing at audio rate versus control rate, and how the level of detail in the signalâand the complexity of the mappingsâinfluence the experience of control in the instrument. This is exemplified through reflections on four concerts in which RAW has been used in different types of collective improvisation.","ÃaÄrÄ± Erdem, and Alexander Refsum Jensenius",
Melia: An Expressive Harmonizer at the Limits of AI,https://www.nime.org/proc/nime2025_93/index.html ,,,,NIME,2025,"We present Melia, a digital harmonizer instrument that explores how common failure modes of machine learning and artificial intelligence (ML/AI) systems can be used in expressive and musical ways. The instrument is anchored by an audio-to-audio neural network trained on a hand-curated dataset to perform pitch-shifting and dynamic filtering. Biased training data and poor out-of-distribution generalization are deliberately leveraged as musical devices and sources of instrument-defining idiosyncrasies. Melia features a custom hardware interface with a MIDI keyboard that polyphonically allocates instances of the model to harmonize live audio input, as well as controls that manipulate model parameters and various audio effects in real-time. This paper presents an overview of related work, the instrument itself, and a discussion of how audio-to-audio AI models might fit into the long-standing tradition of musicians, artists, and instrument-makers finding inspiration in a medium's shortcomings.","Matthew Caren, and Joshua Bennett",
Muscle-Guided Guitar Pedalboard: Exploring Interaction Strategies Through Surface Electromyography and Deep Learning,https://www.nime.org/proc/nime2024_37/index.html ,,,,NIME,2024,"This paper explores a method to innovate the conventional interaction with a guitar pedalboard. By analyzing muscular contractions tracked via surface Electromyography (sEMG) wearable sensors, we aimed to investigate how to dynamically track guitaristsâ sonic intentions to automatically control the guitar sound. Two Recurrent Neural Networks based on Bidirectional Long-Short Term Memory were developed to analyze sEMG signals in real-time. The system was designed as a digital musical instrument that calibrates itself to each user during an initial training process. During training musicians provide their gestural vocabulary, associating each gesture to a corresponding pedalboard preset. The selection of the most effective features, in synergy with the best set of muscles, was conducted to optimize the learning rate of the system. The system was assessed with a user study encompassing seven expert guitar players. Results showed that, on average, participants appreciated the concept underlying the system and deemed it to be able to foster their creativity.","Davide Lionetti, Luca Turchet, Massimiliano Zanoni, and Paolo Belluco",
"eTud,be: case studies in playing with musical agents",https://www.nime.org/proc/nime2023_39/index.html ,,,,NIME,2023,"The eTud,be framework adapts existing improvising musical agents (MA) for performance with an augmented instrument called the eTube. This instrument has been developed with deliberate musical and technological limitations including a simple two-button controller and restricted pitch capacity. We will present case studies which outline our research-creation framework for mapping the eTube controller, developing corpora for the MAs, and testing interactive and machine listening settings which will also be demonstrated by performance examples. A general summary of the MAs will be followed by specific descriptions of the features we have utilised in our work, and finally a comparison of the MAs based on these features. Few papers discuss the process for learning to work with and adapt existing MAs and we will finish by describing challenges experienced as other users with these technologies.","Tommy Davis, Kasey LV Pocius, Vincent Cusson, Marcelo Wanderley, and Philippe Pasquier",
A Human-Agents Music Performance System in an Extended Reality Environment,https://www.nime.org/proc/nime2023_2/index.html ,,,,NIME,2023,"This paper proposes a human-machine interactive music system for live performances based on autonomous agents, implemented through immersive extended reality. The interaction between humans and agents is grounded in concepts related to Swarm Intelligence and Multi-Agent systems, which are reflected in a technological platform that involves a 3D physical-virtual solution. This approach requires visual, auditory, haptic, and proprioceptive modalities, making it necessary to integrate technologies capable of providing such a multimodal environment. The prototype of the proposed system is implemented by combining Motion Capture, Spatial Audio, and Mixed Reality technologies. The system is evaluated in terms of objective measurements and tested with users through music improvisation sessions. The results demonstrate that the system is used as intended with respect to multimodal interaction for musical agents. Furthermore, the results validate the novel design and integration of the required technologies presented in this paper.","Pedro P Lucas, and Stefano Fasciani",
An Exploration of Peg Solitaire as a Compositional Tool,https://www.nime.org/proc/kkeatch2014/index.html ,,,,NIME,2014,"Sounds of Solitaire is a novel interface for musical expression based on an extended peg solitaire board as a generator of live musical composition. The classic puzzle game, for one person, is extended by mapping the moves of the game through a self contained system using Arduino and Raspberry Pi, triggering both analogue and digital sound. The solitaire board, as instrument, is presented as a wood and Perspex box with the hardware inside. Ball bearings function as both solitaire pegs and switches, while a purpose built solenoid controlled monochord and ball bearing run provide the analogue sound source, which is digitally manipulated in real-time, according to the sequences of game moves. The creative intention of Sounds of Solitaire is that the playful approach to participation in a musical experience, provided by the material for music making in real-time, demonstrates an integrated approach to concepts of composing, performing and listening.",Kirsty Keatch,
ism: Improvisation Supporting System based on Melody Correction,https://www.nime.org/proc/ishida2004/index.html ,,,,NIME,2004,"In this paper, we describe a novel improvisation supporting system based on correcting musically unnatural melodies. Since improvisation is the musical performance style that involves creating melodies while playing, it is not easy even for the people who can play musical instruments. However, previous studies have not dealt with improvisation support for the people who can play musical instruments but cannot improvise. In this study, to support such players' improvisation, we propose a novel improvisation supporting system called ism, which corrects musically unnatural melodies automatically. The main issue in realizing this system is how to detect notes to be corrected (i.e., musically unnatural or inappropriate). We propose a method for detecting notes to be corrected based on the N-gram model. This method first calculates N-gram probabilities of played notes, and then judges notes with low N-gram probabilities to be corrected. Experimental results show that the N-gram-based melody correction and the proposed system are useful for supporting improvisation.","Katsuhisa Ishida, Tetsuro Kitahara, and Masayuki Takeda",
Real-time Adaptive Control of Modal Synthesis,https://www.nime.org/proc/hoskinson2003/index.html ,,,,NIME,2003,"We describe the design and implementation of an adaptive system to map control parameters to modal audio synthesis parameters in real-time. The modal parameters describe the linear response of a virtual vibrating solid, which is played as a musical instrument by a separate interface. The system uses a three layer feedforward backpropagation neural network which is trained by a discrete set of input-output examples. After training, the network extends the training set, which functions as the specification by example of the controller, to a continuous mapping allowing the real-time morphing of synthetic sound models. We have implemented a prototype application using a controller which collects data from a hand-drawn digital picture. The virtual instrument consists of a bank of modal resonators whose frequencies, dampings, and gains are the parameters we control. We train the system by providing pictorial representations of physical objects such as a bell or a lamp, and associate high quality modal models obtained from measurements on real objects with these inputs. After training, the user can draw pictures interactively and âplayâ modal models which provide interesting (though unrealistic) interpolations of the models from the training set in real-time.","Reynald Hoskinson, Kees van den Doel, and Sidney S. Fels",
GestureRNN:  A neural gesture system for the Roli Lightpad Block,https://www.nime.org/proc/hantrakul2018/index.html ,,,,NIME,2018,"Machine learning and deep learning has recently made a large impact in the artistic community. In many of these applications however, the model is often used to render the high dimensional output directly e.g. every individual pixel in the final image. Humans arguably operate in much lower dimensional spaces during the creative process e.g. the broad movements of a brush. In this paper, we design a neural gesture system for music generation based around this concept. Instead of directly generating audio, we train a Long Short Term Memory (LSTM) recurrent neural network to generate instantaneous position and pressure on the Roli Lightpad instrument. These generated coordinates in turn, give rise to the sonic output defined in the synth engine. The system relies on learning these movements from a musician who has already developed a palette of musical gestures idiomatic to the Lightpad. Unlike many deep learning systems that render high dimensional output, our low-dimensional system can be run in real-time, enabling the first real time gestural duet of its kind between a player and a recurrent neural network on the Lightpad instrument.",Lamtharn Hantrakul,
Automatic Recognition of Soundpainting for the Generation of Electronic Music Sounds,https://www.nime.org/proc/gomezjauregui2019/index.html ,,,,NIME,2019,"This work aims to explore the use of a new gesture-based interaction built on automatic recognition of Soundpainting structured gestural language. In the proposed approach, a composer (called Soundpainter) performs Soundpainting gestures facing a Kinect sensor. Then, a gesture recognition system captures gestures that are sent to a sound generator software. The proposed method was used to stage an artistic show in which a Soundpainter had to improvise with 6 different gestures to generate a musical composition from different sounds in real time. The accuracy of the gesture recognition system was evaluated as well as Soundpainter's user experience. In addition, a user evaluation study for using our proposed system in a learning context was also conducted. Current results open up perspectives for the design of new artistic expressions based on the use of automatic gestural recognition supported by Soundpainting language.","David Antonio GÃ³mez JÃ¡uregui, Irvin Dongo, and Nadine Couture",
An Interface for Live Interactive Sonification,https://www.nime.org/proc/ferguson2009/index.html ,,,,NIME,2009,"Sonification is generally considered in a statistical data analysis context. This research discusses the development of an interface for live control of sonification â for controlling and altering sonifications over the course of their playback. This is designed primarily with real-time sources in mind, rather than with static datasets, and is intended as a performative, live data-art creative activity. The interface enables the performer to use the interface as an instrument for iterative interpretations and variations of sonifications of multiple datastreams. Using the interface, the performer can alter the scale, granularity, timbre, hierarchy of elements, spatialisation, spectral filtering, key/modality, rhythmic distribution and register âon-the-flyâ to both perform data-generated music, and investigate data in a live exploratory, interactive manner.","Sam Ferguson, and Kirsty Beilharz",
An Agent-based System for Robotic Musical Performance,https://www.nime.org/proc/eigenfeldt2008/index.html ,,,,NIME,2008,"This paper presents an agent-based architecture for robotic musical instruments that generate polyphonic rhythmic patterns that continuously evolve and develop in a musically ""intelligent"" manner. Agent-based software offers a new method for real-time composition that allows for complex interactions between individual voices while requiring very little user interaction or supervision. The system described, Kinetic Engine, is an environment in which individual software agents, emulate drummers improvising within a percussion ensemble. Player agents assume roles and personalities within the ensemble, and communicate with one another to create complex rhythmic interactions. In this project, the ensemble is comprised of a 12-armed musical robot, MahaDeviBot, in which each limb has its own software agent controlling what it performs.","Arne Eigenfeldt, and Ajay Kapur",
TweetDreams : Making Music with the Audience and the World using Real-time Twitter Data,https://www.nime.org/proc/dahl2011/index.html ,,,,NIME,2011,"TweetDreams is an instrument and musical compositionwhich creates real-time sonification and visualization oftweets. Tweet data containing specified search terms is retrieved from Twitter and used to build networks of associated tweets. These networks govern the creation of melodiesassociated with each tweet and are displayed graphically.Audience members participate in the piece by tweeting,and their tweets are given special musical and visual prominence.","Luke Dahl, Jorge Herrera, and Carr Wilkerson",
Network Jamming : Distributed Performance using Generative Music,https://www.nime.org/proc/brown2010/index.html ,,,,NIME,2010,"Generative music systems can be played by musicians who manipulate the values of algorithmic parameters, and their datacentric nature provides an opportunity for coordinated interaction amongst a group of systems linked over IP networks; a practice we call Network Jamming. This paper outlines the characteristics of this networked performance practice and discusses the types of mediated musical relationships and ensemble configurations that can arise. We have developed and tested the jam2jam network jamming software over recent years. We describe this system, draw from our experiences with it, and use it to illustrate some characteristics of Network Jamming.",Andrew R. Brown,
Musical Micro-Timing for Live Coding,https://doi.org/10.5281/zenodo.10265231,,,,ISMIR,2023,"Micro-timing is an essential part of human music-making, yet it is absent from most computer music systems. Partly to address this gap, we present a novel system for generating music with style-specific micro-timing within the Sonic Pi live coding language. We use a probabilistic approach to control the exact timing according to patterns discovered in new analyses of existing micro-timing data (jembe drumming and Viennese waltz). This implementation also required the introduction of musical metre into Sonic Pi. The new metre and micro-timing systems are inherently flexible, and thus open to a wide range of creative possibilities including (but not limited to): creating new micro-timing profiles for additional styles; expanded definitions of metre; and the free mixing of one micro-timing style with the musical content of another. The code is freely available as a Sonic Pi plug-in and released open source at https://github.com/MaxTheComputerer/sonicpi-metre.","Max Johnson, Mark R. H. Gotham",
"Collaboration Between Robots, Interfaces and Humans: Practice-Based and Audience Perspectives",https://arxiv.org/pdf/2407.16966,,,,ICMC,2024,"This paper provides an analysis of a mixed-media experimental musical work that explores the integration of human musical interaction with a newly developed interface for the violin, manipulated by an improvising violinist, interactive visuals, a robotic drummer and an improvised synthesised orchestra. We first present a detailed technical overview of the systems involved including the design and functionality of each component. We then conduct a practice-based review examining the creative processes and artistic decisions underpinning the work, focusing on the challenges and breakthroughs encountered during its development. Through this introspective analysis, we uncover insights into the collaborative dynamics between the human performer and technological agents, revealing the complexities of blending traditional musical expressiveness with artificial intelligence and robotics. To gauge public reception and interpretive perspectives, we conducted an online survey, sharing a video of the performance with a diverse audience. The feedback collected from this survey offers valuable viewpoints on the accessibility, emotional impact, and perceived artistic value of the work. Respondents’ reactions underscore the transformative potential of integrating advanced technologies in musical performance, while also highlighting areas for further exploration and refinement.","Anna Savery, Richard Savery",
REVIVE: An Audio-visual Performance with Musical and Visual AI Agents,https://dl.acm.org/doi/10.1145/3170427.3177771,,,,CHI,2018,"REVIVE explores the affordances of live interaction between the artificial musical agent MASOM, human electronic musicians, and visual generation agents. The Musical Agent based on Self-Organizing Maps (MASOM) has memorized sound objects and learned how to temporally structure them by listening to large corpora of human-made music. MASOM is then able to improvise live interacting with the other (human) performers by imitating the style of what it reminds it of. For each musician, a corresponding visual agent puts its sound and musical decision into images thus allowing the audience to see who does what. This reveals the musical gestures that are so often lost in electronic music performance. For CHI, MASOM plays with two live performers for a 20 minute audiovisual REVIVE experience.","Kıvanç Tatar, Philippe Pasquier, Remy Siu",
JamSketch: A Drawing-based Real-time Evolutionary Improvisation Support System,https://www.nime.org/proc/tkitahara2017/index.html,,,,NIME,2017,"In this paper, we present JamSketch, a real-time improvisation support system which automatically generates melodies according to melodic outlines drawn by the users. The system generates the improvised melodies based on (1) an outline sketched by the user using a mouse or a touch screen, (2) a genetic algorithm based on a dataset of existing music pieces as well as musical knowledge, and (3) an expressive performance model for timing and dynamic transformations. The aim of the system is to allow people with no prior musical knowledge to be able to enjoy playing music by improvising melodies in real time.","Tetsuro Kitahara, Sergio Giraldo, and Rafael RamÃ­rez",
RhumbLine: Plectrohyla Exquisita â Spatial Listening of Zoomorphic Musical Robots,https://www.nime.org/proc/nime21_79/index.html,,,,NIME,2021,"Contending with ecosystem silencing in the Anthropocene, RhumbLine: Plectrohyla Exquisita is an installation-scale instrument featuring an ensemble of zoomorphic musical robots that generate an acoustic soundscape from behind an acousmatic veil, highlighting the spatial attributes of acoustic sound. Originally conceived as a physical installation, the global COVID-19 pandemic catalyzed a reconceptualization of the work that allowed it to function remotely and collaboratively with users seeding robotic frog callers with improvised rhythmic calls via the internetâtransforming a physical installation into a web-based performable installation-scale instrument. The performed calls from online visitors evolve using AI as they pass through the frog collective. After performing a rhythm, audiences listen ambisonically from behind a virtual veil and attempt to map the formation of the frogs, based on the spatial information embedded in their calls. After listening, audience members can reveal the frogs and their formation. By reconceiving rhumb linesânavigational tools that create paths of constant bearing to navigate spaceâas sonic tools to spatially orient listeners, RhumbLine: Plectrohyla Exquisita functions as a new interface for spatial musical expression (NISME) in both its physical and virtual instantiations.","Margaret Schedel, Brian Smith, Robert Cosgrove, and Nick Hwang",
A Laptop Ensemble Performance System using Recurrent Neural Networks,https://www.nime.org/proc/nime20_9/index.html,,,,NIME,2020,"The popularity of applying machine learning techniques in musical domains has created an inherent availability of freely accessible pre-trained neural network (NN) models ready for use in creative applications. This work outlines the implementation of one such application in the form of an assistance tool designed for live improvisational performances by laptop ensembles. The primary intention was to leverage off-the-shelf pre-trained NN models as a basis for assisting individual performers either as musical novices looking to engage with more experienced performers or as a tool to expand musical possibilities through new forms of creative expression. The system expands upon a variety of ideas found in different research areas including new interfaces for musical expression, generative music and group performance to produce a networked performance solution served via a web-browser interface. The final implementation of the system offers performers a mixture of high and low-level controls to influence the shape of sequences of notes output by locally run NN models in real time, also allowing performers to define their level of engagement with the assisting generative models. Two test performances were played, with the system shown to feasibly support four performers over a four minute piece while producing musically cohesive and engaging music. Iterations on the design of the system exposed technical constraints on the use of a JavaScript environment for generative models in a live music context, largely derived from inescapable processing overheads.","Rohan Proctor, and Charles Patrick Martin",
Gesture-Driven DDSP Synthesis for Digitizing the Chinese Erhu,https://www.nime.org/proc/nime2025_73/index.html,,,,NIME,2025,"This paper presents a gesture-controlled digital Erhu system that merges traditional Chinese instrumental techniques with contemporary machine learning and interactive technologies. By leveraging the Erhuâs expressive techniques, we develop a dual-hand spatial interaction framework using real-time gesture tracking. Hand movement data is mapped to sound synthesis parameters to control pitch, timbre, and dynamics, while a differentiable digital signal processing (DDSP) model, trained on a custom Erhu dataset, transforms basic waveforms into authentic timbre which remians sincere to the instrumentâs nuanced articulations. The system bridges traditional musical aesthetics with digital interactivity, emulating Erhu bowing dynamics and expressive techniques through embodied interaction. The study contributes a novel framework for digitizing Erhu performance practices, explores methods to align culturally informed gestures with DDSP-based synthesis, and offers insights into preserving traditional instruments within digital music interfaces.","Wenqi WU, and Hanyu QU",
Real-Time Co-Creation of Expressive Music Performances Using Speech and Gestures,https://www.nime.org/proc/nime2023_91/index.html,,,,NIME,2023,"We present a system for interactive co-creation of expressive performances of notated music using speech and gestures. The system provides real-time or near-real-time dialog-based control of performance rendering and interaction in multiple modalities. It is accessible to people regardless of their musical background via smartphones. The system is trained using sheet music and associated performances, in particular using notated performance directions and user-system interaction data to ground performance directions in performances. Users can listen to an autonomously generated performance or actively engage in the performance process. A speech- and gesture-based feedback loop and online learning from past user interactions improve the accuracy of the performance rendering control. There are two important assumptions behind our approach: a) that many people can express nuanced aspects of expressive performance using natural human expressive faculties, such as speech, voice, and gesture, and b) that by doing so and hearing the music follow their direction with low latency, they can enjoy playing the music that would otherwise be inaccessible to them. The ultimate goal of this work is to enable fulfilling and accessible music making experiences for a large number of people who are not currently musically active.","Ilya Borovik, and Vladimir Viro",
ImprovGenerator : Online Grammatical Induction for On-the-Fly Improvisation Accompaniment,https://www.nime.org/proc/kitani2010/index.html,,,,NIME,2010,"We propose an online generative algorithm to enhance musical expression via intelligent improvisation accompaniment.Our framework called the ImprovGenerator, takes a livestream of percussion patterns and generates an improvisedaccompaniment track in real-time to stimulate new expressions in the improvisation. We use a mixture model togenerate an accompaniment pattern, that takes into account both the hierarchical temporal structure of the liveinput patterns and the current musical context of the performance. The hierarchical structure is represented as astochastic context-free grammar, which is used to generateaccompaniment patterns based on the history of temporalpatterns. We use a transition probability model to augmentthe grammar generated pattern to take into account thecurrent context of the performance. In our experiments weshow how basic beat patterns performed by a percussioniston a cajon can be used to automatically generate on-the-flyimprovisation accompaniment for live performance.","Kris M. Kitani, and Hideki Koike",
Generating an Integrated Musical Expression with a Brain--Computer Interface,https://www.nime.org/proc/hamano2013/index.html,,,,NIME,2013,"Electroencephalography (EEG) has been used to generate music for over 40 years,but the most recent developments in brain--computer interfaces (BCI) allowgreater control and more flexible expression for using new musical instrumentswith EEG. We developed a real-time musical performance system using BCItechnology and sonification techniques to generate imagined musical chords withorganically fluctuating timbre. We aim to emulate the expressivity oftraditional acoustic instruments. The BCI part of the system extracts patternsfrom the neural activity while a performer imagines a score of music. Thesonification part of the system captures non-stationary changes in the brainwaves and reflects them in the timbre by additive synthesis. In this paper, wediscuss the conceptual design, system development, and the performance of thisinstrument.","Takayuki Hamano, Tomasz Rutkowski, Hiroko Terasawa, Kazuo Okanoya, and Kiyoshi Furukawa",
"A Knowledge-based, Data-driven Method for Action-sound Mapping",https://www.nime.org/proc/fvisi2017/index.html,,,,NIME,2017,"This paper presents a knowledge-based, data-driven method for using data describing action-sound couplings collected from a group of people to generate multiple complex mappings between the performance movements of a musician and sound synthesis. This is done by using a database of multimodal motion data collected from multiple subjects coupled with sound synthesis parameters. A series of sound stimuli is synthesised using the sound engine that will be used in performance. Multimodal motion data is collected by asking each participant to listen to each sound stimulus and move as if they were producing the sound using a musical instrument they are given. Multimodal data is recorded during each performance, and paired with the synthesis parameters used for generating the sound stimulus. The dataset created using this method is then used to build a topological representation of the performance movements of the subjects. This representation is then used to interactively generate training data for machine learning algorithms, and define mappings for real-time performance. To better illustrate each step of the procedure, we describe an implementation involving clarinet, motion capture, wearable sensor armbands, and waveguide synthesis.","Federico Visi, Baptiste Caramiaux, Michael Mcloughlin, and Eduardo Miranda",
Motivated Learning in Human-Machine Improvisation,https://www.nime.org/proc/beyls2018/index.html,,,,NIME,2018,"This paper describes a machine learning approach in the context of non-idiomatic human-machine improvisation. In an attempt to avoid explicit mapping of user actions to machine responses, an experimental machine learning strategy is suggested where rewards are derived from the implied motivation of the human interactor â two motivations are at work: integration (aiming to connect with machine generated material) and expression (independent activity). By tracking consecutive changes in musical distance (i.e. melodic similarity) between human and machine, such motivations can be inferred. A variation of Q-learning is used featuring a self-optimizing variable length state-action-reward list. The system (called Pock) is tunable into particular behavioral niches by means of a limited number of parameters. Pock is designed as a recursive structure and behaves as a complex dynamical system. When tracking systems variables over time, emergent non-trivial patterns reveal experimental evidence of attractors demonstrating successful adaptation.",Peter Beyls,
"AMIGO: An Assistive Musical Instrument to Engage, Create and Learn Music",https://www.nime.org/proc/almeida2019/index.html,,,,NIME,2019,"We present AMIGO, a real-time computer music system that assists novice users in the composition process through guided musical improvisation. The system consists of 1) a computational analysis-generation algorithm, which not only formalizes musical principles from examples, but also guides the user in selecting note sequences; 2) a MIDI keyboard controller with an integrated LED stripe, which provides visual feedback to the user; and 3) a real-time music notation, which displays the generated output. Ultimately, AMIGO allows the intuitive creation of new musical structures and the acquisition of Western music formalisms, such as musical notation.","Isabela Corintha Almeida, Giordano Cabral, and Professor Gilberto Bernardes Almeida",
Real-Time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar,https://doi.org/10.5281/zenodo.10265236,,,,ISMIR,2023,"Real-time music information retrieval (RT-MIR) has much potential to augment the capabilities of traditional acoustic instruments. We develop RT-MIR techniques aimed at augmenting percussive fingerstyle, which blends acoustic guitar playing with guitar body percussion. We formulate several design objectives for RT-MIR systems for augmented instrument performance: (i) causal constraint, (ii) perceptually negligible action-to-sound latency, (iii) control intimacy support, (iv) synthesis control support. We present and evaluate real-time guitar body percussion recognition and embedding learning techniques based on convolutional neural networks (CNNs) and CNNs jointly trained with variational autoencoders (VAEs). We introduce a taxonomy of guitar body percussion based on hand part and location. We follow a cross-dataset evaluation approach by collecting three datasets labelled according to the taxonomy. The embedding quality of the models is assessed using KL-Divergence across distributions corresponding to different taxonomic classes. Results indicate that the networks are strong classifiers especially in a simplified 2-class recognition task, and the VAEs yield improved class separation compared to CNNs as evidenced by increased KL-Divergence across distributions. We argue that the VAE embedding quality could support control intimacy and rich interaction when the latent space's parameters are used to control an external synthesis engine. Further design challenges around generalisation to different datasets have been identified.","Andrea Martelloni, Andrew P. McPherson, Mathieu Barthet",
CalmusWaves: Where Dancers Compose Music in Real-Time,https://quod.lib.umich.edu/i/icmc/bbp2372.2017.076/--calmuswaves-where-dancers-compose-music-in-real-time?view=image,,,,ICMC,2017,"CalmusWaves is the first live performance project of its kind where choreography, music and lighting settings are created together in real-time. During CalmusWaves performances, dancers¾connected to the composition program CALMUS through wireless OSC1 sensors¾interact with musicians who sight-read graphical notation appearing simultaneously on iPads via the CALMUS Notation app. Further input is given by light technicians and computer programmers. This process allows the participants to control and modify the development of a real-time composition through their onstage performances [1]. The principal aim of this research project was to investigate the mapping of the dancers’ gestures and movements to control the real-time musical composition. Other research topics included how the data deriving from the wireless sensors could be converted, filtered and truncated into data allowing CALMUS to understand and react to the stimuli in a musical way. The purpose of this paper is to describe the technical and artistic processes involved in our project.","Jen Webb, Paul Munden",
PROMPT JOCKEYS (2024) - The Rise of DJing with a Neural Network,https://www.youtube.com/watch?v=_fpnAHoRSqU ,,,,Video,2024,,DADABOTS,
The Flaming Lips at Google I/O,https://magenta.tensorflow.org/fruitgenie ,,,,Video,2023,,Deeplocal,
hexorcismos - SEMILLA A.I.,https://www.youtube.com/watch?v=_2C3XeQgGtY,,,,Video,2023,,hexorcismos,
"Magnetologues - for two Stacco, Neural Synth and Ambisonics",https://www.youtube.com/watch?v=Bt3O-jhSqiU&t=75s,,,,Video,2024,,Giacomo Lepri,
Sophtar live at the Guthman Musical Instrument Competition 2025,https://www.youtube.com/watch?v=lpw6pRQltrY&list=RDlpw6pRQltrY&start_radio=1,,,,Video,2025,,Federico Visi,
Umwelt Synthesis | Rob Clouth | OpenLAB#03,https://www.youtube.com/watch?v=a6jmnD-c9lo,,,,Video,2024,,Medialab Matadero,
BIGYUKI × Qosmo — John Connor Project / 2023.8.22 BAROOM,https://youtu.be/N5ZumP76f4c?feature=shared,https://qosmo.jp/en/art/bigyuki,,,Video,2023,,Nao Tokui,
Live Music Performance using A.I. - Trumpet & Electronics,https://www.youtube.com/watch?v=ETNmG_dUcZ8,,,,Video,2023,,Cameron Summers,
How IU professors use AI to improvise music in their studio,https://www.youtube.com/watch?v=hA_Cq3qcHCM,,,,Video,2024,,WRTV Indianapolis,
Joint improvisation between human and AI,https://youtu.be/sIFbvgmYBA0?feature=shared,,,,Video,2022,,Obed Ben-Tal,
Piano AI Improvisation - Single Instrument sample,https://youtu.be/loztbOaOWn8?feature=shared ,,,,Video,2018,,David Samuel,
Harmonizing with AI: An Unprecedented Musical Improvisation! (Chat GPT 4),https://www.youtube.com/watch?v=tVfAfKBWt5k&list=RDtVfAfKBWt5k&start_radio=1,,,,Video,2023,,Sunheart,
AI & Music Improvisation. Research collaboration Melbourne/London,https://www.youtube.com/watch?v=GrM_Ls3PnfA,,,,Video,2021,,Sensilab monash,
Musical Improvisation with AI (15:10-28:25),https://www.youtube.com/watch?v=NnBnCj_LSV8,,,,Video,2021,,ARTIFICIA,
AI DJ Project #2 Ubiquitous Rhythm — A Spontaneous Jam Session with AI,https://www.youtube.com/watch?v=XM02IT00rq8,,,,Video,2021,,Qosmo — AI Creativity & Music Lab ,
Magenta AI Jam Session,https://www.youtube.com/watch?v=QlVoR1jQrPk ,https://www.youtube.com/watch?v=ZRnbbtqxBEc ,,,Video,2016,,Magenta,
AI Jam Session: Vibe-Coding meets Live-Coding Music with Emergentic.ai in Strudel,https://www.youtube.com/watch?v=u6NJCSC5SP8,,,,Video,2025,,The Garden in the Machine,
Jam Session With My AI,https://www.youtube.com/watch?v=Cxim3MFILW8,,,,Video,2024,,PeteyPandaBoy,
Live-coding music with a Deep Neural Network,https://www.youtube.com/watch?v=6ekNzbftZcQ,,,,Video,2017,,Circu Virtu,
Exploring Real-Time Biomedical Data Science and Gesture-Driven Music Generation | BioniChaos,https://www.youtube.com/watch?v=_wZDdtfjqNg ,,,,Video,2024,,BioniChaos,
【AI Music】Real-time music generation on everyone's comments! [Lofi/Chill radio]22,https://www.youtube.com/watch?v=4TDCN1SCew0,https://youtube.com/shorts/kb8Pes6BSGg?feature=shared  ,https://youtube.com/shorts/k90dQR4ohLg?feature=shared,,Video,2024,,DAJI - AI Music Channel EN,
"Drumnose (with closeups) in real-time, algorithmic music/ rhythm generation (#2)",https://www.youtube.com/watch?v=YexnsjonT2U,,,,Video,2015,,dudenose,
Improvisation with AI,https://www.youtube.com/watch?v=WruN6En74FU,,,,Video,2023,,Enric Guaus,Enric Guaus
"Intersections // Man, Machine & Improvisation",https://www.youtube.com/watch?v=mWKK-_qUSBE,,,,Video,2012,,Jerry Fleming,
Uncanny Love - An AI Driven Improvisation on Human Machine Bonds,https://www.youtube.com/watch?v=WqhgmudVAc4&list=RDWqhgmudVAc4&start_radio=1,,,,Video,2023,,sonic space basel,
Me/Machine - real-time improvisational interaction between a computer and a musician,https://www.youtube.com/watch?v=XaqTkLI4DxQ,,,,Video,2023,,AI for Good,
Realtime Neural Audio Synthesis - embedded RAVE #2,https://www.youtube.com/watch?v=jAIRf4nGgYI,,,,Video,2022,,Acids Team - Icram,
Neurorack - The first deep AI-based Eurorack synthesizer,https://www.youtube.com/watch?v=64VpQenCHVs,,,,Video,2022,,Acids Team - Icram,
Approaches to Musical Expression in Harmonix Video Games (Fantasia),https://www.worldscientific.com/doi/10.1142/9789813140103_0002?srsltid=AfmBOorCP36E6mpdFgxRfNQrCdFoyznuaVuTu2-Vp-d2Kjbg7kEtc5o5,,,,Mathemusical Conversations: Mathematics and Computation in Music Performance and Composition,2016,"Harmonix Music Systems was founded in 1995 with the mission “to let everyone in the world experience the joy of making music.” Over the past 20 years, Harmonix has released over 20 game titles that, to various degrees of success, have let players feel musical and express themselves musically. This chapter explores five Harmonix games, each of which has taken a different approach to enable player musical expression: The Axe (a joystick-controlled improvisation system), Amplitude (an arcade-style multi-track rhythm game), Karaoke Revolution (a microphone-based singing game), Guitar Hero and Rock Band (multi-instrument rock band simulation game), and Fantasia: Music Evolved (Kinect-based musical exploration game based on the classic Disney movie). It addresses a central dilemma in designing music video games: can players enjoy being expressive while also driving towards the goal of “winning the game?”",Eran Egozy,
Composing the Assemblage: Probing Aesthetic and Technical Dimensions of Artistic Creation with Machine Learning (Case 2),https://watermark02.silverchair.com/comj_a_00658.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAzowggM2BgkqhkiG9w0BBwagggMnMIIDIwIBADCCAxwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM7ywKGsiS99fY-K7yAgEQgIIC7ULMaiRpyOyojBfhd_txkYfJ1aBEo3Kw9WWk-YOASJ_PApCtM33ew6LulWGVjf_nj-3WZ4iNbh3AI-fa6IajUUKxH2hCn4bCSo1jhrtuEmthby8K7rm8eX1RoiSZCoCRyxchu-WLIarB8zycoEF_VETiP5Qo55dfWwhQ9YNvNx9rTQflFvljhss4ooWhC9_4Gzgeaex3z3Weos2Vv4OWKtZh2esoAcZallj0C4nYqy0gaEOMIuLMIhYf1SMnrDpfbt-DZNm7Y1D8T8bJIPqf_HR6zlcBTIInJfJ6dof-n7nHFCk80gMeBUgSKrBBXCKDhYN3cqgvgSdAedsKVQTMTW0vUwhHPHKBlihL-hURc3vOOtnH1-6Jlmeh9MZsR3vq7db2UQoAEumdW8FvR1qVJRymbdwNLoq21OA8SLkNN8cCr5X-744_PqUsCsRZwO5XeyUBfQO0yM3blEbcSDYrrIogf1OcfUKGue-9bs9ozsYmuuslSmYv6sS9H4GXdMMqtP9ICQ4us6zudP9UAmPJcG5jccg9mfh5VLKvwcxYp8e5mWI8xwwNb_gcW90xMUGPaizjDfJBg6m2MX2a7tJ32PDYwXoJsOUHUFmXeCX9xA2kQ5dRHW1RTjuhhPhZbn5mJr4KXW3gPhaXcpekUNdMhT84rVrAU7N3gp9_dgEg9qacmtq-BC8bPBZwV21R-UtLennP_q7uE59g_-m9vIRqgWRZ_KOloESnhzflHTNS8oxSGPxG4DGxqUL6KCkJex4GsQ1cWogeJ-n1I-M1qR1Gp0rmyl55fch9pqxCLzSDGrGhC25tg5RbcoNMd_CDCaYdVJYHQmE4hRhIaB0-g5j87G0RMz8xKa82s9F0KBWwiTODl7wnQE8PI59_60Kg9krbD3u_UUReC70e4Fp4NywWTBz0bV84ZsjwObzN76jZsUuajaHbbhftYU351a7mSESjsSqyL8kWclFEYGfb6uD204c9G428J9GThgXW3gD3,,,,Computer Music Journal,2022,"In this article, we address the role of machine learning (ML) in the composition of two new musical works for acoustic instruments and electronics through autoethnographic reflection on the experience. Our study poses the key question of how ML shapes, and is in turn shaped by, the aesthetic commitments characterizing distinctive compositional practices. Further, we ask how artistic research in these practices can be informed by critical themes from humanities scholarship on material engagement and critical data studies. Through these frameworks, we consider in what ways the interaction with ML algorithms as part of the compositional process differs from that with other music technology tools. Rather than focus on narrowly conceived ML algorithms, we take into account the heterogeneous assemblage brought into play: from composers, performers, and listeners to loudspeakers, microphones, and audio descriptors. Our analysis focuses on a deconstructive critique of data as being contingent on the decisions and material conditions involved in the data-creation process. It also explores how interaction among the human and nonhuman collaborators in the ML assemblage has significant similarities to—as well as differences from—existing models of material engagement. Tracking the creative process of composing these works, we uncover the aesthetic implications of the many nonlinear collaborative decisions involved in composing the assemblage.","Artemi-Maria Gioti, Aaron Einbond, Georgina Born",
Ircam's Dicy2 generative musical agents - Some playing modes,https://www.youtube.com/watch?v=i87scN-mtUE,,,,Video,2024,,Jérôme Nika,
Following an Improvisation in Real Time,https://quod.lib.umich.edu/cgi/p/pod/dod-idx/following-an-improvisation-in-real-time.pdf?c=icmc;idno=bbp2372.1987.035;format=pdf,,,,ICMC,1987,"A basic level of music understanding is required to recognize the correspondence between jazz improvisations and their underlying chord progressions. In this study, techniques are examined for building a computer system that can recognize and follow a jazz solo by first deducing where beats fall in relation to the solo, and then by using a statistical matching method to find the most likely relationship to a chord progression.","Roger B. Dannenberg, Bernard Mont-Reynaud",